<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="入门,">





  <link rel="alternate" href="/atmol.xml" title="Hello,World" type="application/atom+xml">






<meta name="description" content="Inception系列有四篇重要的paper，分别是：Going Deeper with Convolutions、Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shif、Rethinking the Inception Architecture for Compute">
<meta name="keywords" content="入门">
<meta property="og:type" content="article">
<meta property="og:title" content="Inception">
<meta property="og:url" content="http://blog.hangcheers.cn/2018/12/16/hello-world/index.html">
<meta property="og:site_name" content="Hello,World">
<meta property="og:description" content="Inception系列有四篇重要的paper，分别是：Going Deeper with Convolutions、Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shif、Rethinking the Inception Architecture for Compute">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://camo.githubusercontent.com/842ee01499b72c4d581c3ceaa3c5ff1bba52795a/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f323030302f312a6171347463426c3974355a33366b5444655a534f48412e706e67">
<meta property="og:image" content="https://camo.githubusercontent.com/af3901794f951a95a46683687fa7a60f3d0f45d6/687474703a2f2f63733233316e2e6769746875622e696f2f6173736574732f636e6e2f6465707468636f6c2e6a706567">
<meta property="og:image" content="https://mohitjainweb.files.wordpress.com/2018/06/googlenet-architecture-showing-the-side-connection.png?w=700">
<meta property="og:image" content="https://camo.githubusercontent.com/f038213d9fa7c4c47ca36d7e51820f68462285d2/68747470733a2f2f696d672d626c6f672e6373646e2e6e65742f32303137303631323131303435383434343f77617465726d61726b2f322f746578742f6148523063446f764c324a736232637559334e6b626935755a585176625746796332706f5957383d2f666f6e742f3561364c354c32542f666f6e7473697a652f3430302f66696c6c2f49304a42516b46434d413d3d2f646973736f6c76652f37302f677261766974792f536f75746845617374">
<meta property="og:image" content="https://tse2.mm.bing.net/th?id=OIP.lEHUl5w_rweJSb-FpNdKeAHaFl&pid=Api">
<meta property="og:image" content="http://davidstutz.de/wordpress/wp-content/uploads/2017/03/inception_arch_3.png">
<meta property="og:image" content="https://camo.githubusercontent.com/6766d20252ed7afb1b3e17d92f38020da2328f5e/68747470733a2f2f696d672d626c6f672e6373646e2e6e65742f32303137303232303230313132383933383f77617465726d61726b2f322f746578742f6148523063446f764c324a736232637559334e6b626935755a58517664334e77596d453d2f666f6e742f3561364c354c32542f666f6e7473697a652f3430302f66696c6c2f49304a42516b46434d413d3d2f646973736f6c76652f37302f677261766974792f43656e746572">
<meta property="og:image" content="https://camo.githubusercontent.com/8c540abfc8714b56027ee7545f56967263c1b74a/68747470733a2f2f706963312e7a68696d672e636f6d2f76322d34393234373065313638376531653534373135366565393033363462346636305f722e6a7067">
<meta property="og:image" content="https://camo.githubusercontent.com/ade4e987c83d1aa34417a65032bfa073112ebb39/68747470733a2f2f706963322e7a68696d672e636f6d2f38302f76322d64363535373237343964313535336639636366353062636136353961366666645f68642e6a7067">
<meta property="og:updated_time" content="2018-12-16T05:10:30.439Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Inception">
<meta name="twitter:description" content="Inception系列有四篇重要的paper，分别是：Going Deeper with Convolutions、Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shif、Rethinking the Inception Architecture for Compute">
<meta name="twitter:image" content="https://camo.githubusercontent.com/842ee01499b72c4d581c3ceaa3c5ff1bba52795a/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f323030302f312a6171347463426c3974355a33366b5444655a534f48412e706e67">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://blog.hangcheers.cn/2018/12/16/hello-world/">





  <title>Inception | Hello,World</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-141036651-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hello,World</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.hangcheers.cn/2018/12/16/hello-world/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hang Yang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello,World">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Inception</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T08:13:23+08:00">
                2018-12-16
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-16T13:10:30+08:00">
                2018-12-16
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/论文笔记/" itemprop="url" rel="index">
                    <span itemprop="name">论文笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读数
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4.2k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  17
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <a id="more"></a>
<p>Inception系列有四篇重要的paper，分别是：<a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Going Deeper with Convolutions</a>、<br><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shif</a>、<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision</a>、<a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="noopener">Inception-v4</a><br>在此，依次阅读并做笔记。</p>
<h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><h3 id="Introduction-amp-Motivation"><a href="#Introduction-amp-Motivation" class="headerlink" title="Introduction &amp; Motivation"></a>Introduction &amp; Motivation</h3><p><a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Going Deeper with Convolutions</a> 首次提出了「<strong>Inception</strong>」模块作为网络构架，<br>该网络构架也是后续作为classification和detection的base network的重要组成部分。</p>
<blockquote>
<p>we introduce a new level of organization in the form of the “Inception module” and also in a more direct sense of increased<br>network depth.  </p>
</blockquote>
<p>网络的size主要从两方面进行考虑：depth - the number of levels - of the network 和 width - the number of units at each level。「加深网络depth」、「调节超参数」可以在recognition和object detection取得更好的效果。但是网络的size过大，会直接影响运行的性能。就像一个人过胖，会直接影响身体健康。当网络的size过大的时候，参数#paramters过多，消耗的计算资源就越多，此外，特别是在labeled examples很有限的情况下，更容易出现overfitting。一般是采用「dropout」或者「regularization」，并且「调整超参数」和「设置学习率」去防止训练过程中过拟合现象的出现。</p>
<blockquote>
<p>For larger datasets such as Imagenet, deeper architectures are used to get better results and dropout is used to prevent<br>overfitting …… Since in practice the computational budget is always finite, an efficient distribution of computing resources is preferred to an indiscriminate increase of size。  </p>
</blockquote>
<h3 id="Inception-module"><a href="#Inception-module" class="headerlink" title="Inception module"></a>Inception module</h3><p>上面的方法挺好的，但也挺蛮烦的，所以作者试图结合「数据结构、网络结构」来考虑，如何设计一个创新性的architecture，来更好的利用计算资源以及稍微放心、大胆的设置参数一些?</p>
<p>首先，开个小分支，介绍一下「稀疏结构」的理论基础：<strong>Hebbian原理</strong>。 作者从neuroscience的角度得到了启发，提出了网络结构的创新：。<br>该原理指出：各个神经元是组合效应，通过神经突触进行信息的传递，大脑皮层接收信息。此外，<br>神经反射活动的持续与重复会导致神经元连接稳定性的持久提升，当两个神经元细胞A和B距离很近，并且A参与了对B重复、持续的兴奋，那么某些代谢变化会导致A将作为能使B兴奋的细胞。</p>
<blockquote>
<p>neurons that fire together, wire together.<br>将Fully Connected变为稀疏连接（sparse connection）的时候，可以在增加网络深度和宽度的同时减少参数个数，<br>但是大部分的硬件是针对密集矩阵计算优化的，稀疏矩阵虽然数据量变少，但计算所消耗的时间很难减少。<br>GoogleNet希望做的就是既保证网络结构的稀疏性、又利用密集矩阵的高计算性能。</p>
</blockquote>
<p>GoogleNet的核心是Inception module，而Inception相当于一个Convolutional building block，也是一个局部稀疏最优解的网络构架，然后我们在<br>空间上做堆叠。下面我们结合论文的插图来仔细分析一下Inception module。图a是原始的Inception module，图b是借鉴了NIN（Network In Network）<br>引入1x1的卷积操作，改进后的Inception module。</p>
<blockquote>
<p>Our network will be built from convolutional building blocks.<br>All we need is to find the <strong>optimal local construction</strong> and to repeat it spatially.   </p>
</blockquote>
<p><img src="https://camo.githubusercontent.com/842ee01499b72c4d581c3ceaa3c5ff1bba52795a/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f323030302f312a6171347463426c3974355a33366b5444655a534f48412e706e67" alt="1">  </p>
<p>输入有四个分支，使用多个尺度（1x1或3x3或5x5）的卷积和池化进行特征提取「相当于将稀疏矩阵分解为密集矩阵」，每一尺度提取的特征是均匀分布的，<br>但是经过「filter concatenation」这步操作后，输出的特征不再是均匀分布的，<strong>相关性强的特征会被加强，而相关性弱的特征会被弱化</strong>。<em>这个相关性高的节点应该被连接在一起的结论，即是从神经网络的角度对Hebbian原理有效性的证明</em><br>「filter concatenation」，这一步其实相当于沿着深度方向（或者说在depth这个维度）进行拼接，</p>
<blockquote>
<p>stack up the first volume to the second volume to make the dimensions match up …… Output a single output vector forming the input of<br>next stage。  </p>
</blockquote>
<p>结合<a href="https://becominghuman.ai/understanding-and-coding-inception-module-in-keras-eb56e9056b4b" target="_blank" rel="noopener">Udacity视频</a>和code来加深一下对「filter concatenation」的理解</p>
<blockquote>
</blockquote>
<pre><code>concatenated_tensor = tf.concat(3,[branch1, branch2, branch3, branch 4])  
</code></pre><p><strong>E.g:</strong><br>{General}：输入 28x28x192 volume ，并列经过 1x1卷积操作、3x3卷积操作、5x5卷积操作、max-pool，分别得到28x28x64、28x28x128、<br>28x28x32、28x28x32 volume, 将并列的volume沿着深度方向进行拼接，输出 28x28x256 volume。 </p>
<blockquote>
<p>Feifei-Li的cs231n的课件里是描述CNN的：every layer of a ConvNet transforms one volume of activations to another through a differentiable function.We use three main types of layers to build ConvNet architectures:Convolutional Layer, Pooling Layer,<br>and Fully-Connected Layer.  Conv layer will compute <strong>the output of neurons</strong> that are connected to local regions in the input,<br>each computing a <strong>dot product between their weights</strong> and a small region they are connected to the input volume.<br>Pool layer will perform a downsampling operation along <strong>the spatial dimensions</strong>(<em>width,height</em>)<br>FC layer will compute the class score,resulting in volume of size「1x1x#class」。</p>
</blockquote>
<p>{Specific}：5x5的卷积操作得到了28x28x32的block。<br>filter size =5x5x192，5 pixels width and height, 192 pixels depth（filter的深度需要和<em>前一feature map的深度</em>保持一致。）</p>
<p>设input volume width = W,  the width of receptive field = F_w, zero padding on the border = P, stride = S<br>那么output volume width = (W-F+2P)/S+1。同理也可以得到output volume height。此外, input volume depth = D1<br>此外，被filter覆盖的图像区域称为receptive field，具体操作是：slide each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position，即filter中的值和原始图像中receptive field中的像素值进行点积运算，产生activation map或feature map。<strong>图像一般都是局部相关的</strong>，<br>第n+1层的每个神经元和第n层的receptive field中的神经元连接，而不需要和第n层的所有神经元连接，ConvNet具有<strong>local connectivity(局部连接)</strong> 的性质。当filter的receptive field越大，filter能够处理的原始输入内容的范围就越大。随着经过更多的卷积层，得到的激活映射也就具有更为复杂的特征。  </p>
<p><img src="https://camo.githubusercontent.com/af3901794f951a95a46683687fa7a60f3d0f45d6/687474703a2f2f63733233316e2e6769746875622e696f2f6173736574732f636e6e2f6465707468636f6c2e6a706567" alt="4"></p>
<p>设 number of filters = K, 也是output volume depth的值。当filter的数目越多，spatial dimensions就会保留的越好。<br>CNN具有local connection和parameter sharing的特点。<br>每个filter的权重的个数 = F_w x F_h x D1, 总的权重个数= F_w x F_h x D1 x K</p>
<p>我们再分析一下<strong>compution cost</strong></p>
<blockquote>
<p>cs231n 指出： the largest bottleneck to be aware of when constructing the ConvNet is the memory bottle neck.<br>we need to keep track of the intermediate volume size, the paramter size and the memory.<br><a href="http://cs231n.github.io/convolutional-networks/#conv" target="_blank" rel="noopener">Reference:cs231n</a>  </p>
</blockquote>
<p>现在我们来分析一下，上面的图b相比图a的优势在哪里🧐。<br>1x1的卷积是作为瓶颈层的作用，用很小的计算量可以增加一层特征变换和非线性变换。<em>此外，一般涉及到改变通道数，都会使用1x1卷积操作，<br>例如残差连接和Dense连接</em>  </p>
<blockquote>
<p>the bottleneck is usually the smallest part of something<br>我们来计算一下图a中5x5的卷积操作得到了28x28x32的block的时候，所需要的multiples的次数。以及图b中先使用1x1的卷积操作先得到28x28x16，再使用5x5的卷积操作得到了28x28x32的blcok的时候，所需要的multiples的次数。  </p>
</blockquote>
<p>1.图a (28x28x32) x (5x5x192) = 120million 「一个output volume所需要的乘积次数 x the number of output values」 </p>
<p>2.图b （28x28x16) x (1x1x192) + (28x28x32) x (5x5x16) = 12.4 million  </p>
<p>从上面👆两个对比可以知道1x1的卷积操作大大的减少了计算量。</p>
<h3 id="GoogleNet’s-architecture"><a href="#GoogleNet’s-architecture" class="headerlink" title="GoogleNet’s architecture"></a>GoogleNet’s architecture</h3><p>首先，为了有一个初步的印象，先截取了GoogleNet的一部分，<br><img src="https://mohitjainweb.files.wordpress.com/2018/06/googlenet-architecture-showing-the-side-connection.png?w=700" alt="2"><br>我们可以注意到这里有一个「softmax」的分支，整个结构中有两个「softmax」，它相当于辅助分类器，结合code我们可以知道该操作是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中起到的是梯度前向传输的作用。论文里是这么交代的：</p>
<blockquote>
<p>By adding auxiliary classifiers connected to these intermediate layers, we would expect to encourage discrimination in the lower stages in the classifier, increase the gradient signal that gets propagated back, and provide additional regularization.  </p>
</blockquote>
<p>其次，为了对GoogleNet的组成有一个概念，引用了论文中的表格：<br><img src="https://camo.githubusercontent.com/f038213d9fa7c4c47ca36d7e51820f68462285d2/68747470733a2f2f696d672d626c6f672e6373646e2e6e65742f32303137303631323131303435383434343f77617465726d61726b2f322f746578742f6148523063446f764c324a736232637559334e6b626935755a585176625746796332706f5957383d2f666f6e742f3561364c354c32542f666f6e7473697a652f3430302f66696c6c2f49304a42516b46434d413d3d2f646973736f6c76652f37302f677261766974792f536f75746845617374" alt="3"><br>上面讨论时，已经说过GoogleNet是模块化的，堆叠了多个Inception Module，靠后的Inception Module能够抽取更高阶的抽象的特征。</p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><h3 id="Internal-covariate-shift"><a href="#Internal-covariate-shift" class="headerlink" title="Internal covariate shift"></a>Internal covariate shift</h3><p>“Internal”指的是神经网络的隐含层，”Covariate”指的是输入的权重参数化，“Internal Covariate Shift”指的是<br>在训练的过程中，输入的概率分布不固定，网络的参数在不断的变化，神经网络的隐含层也要不断的去「适应」新的分布。<br>这个现象会让模型更加难训练，我们也需要更加谨慎的初始化模型参数和学习率。因此作者引入了<em>Normalization</em> 来解决这个问题。 </p>
<h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h3><p>通过规范化的手段，将每层神经网络任意神经元这个输入值的分布“强行拉回”到均值为0，方差为1的分布中.常规的正则化公式为：$\hat{x}^{k}=\frac{x^{k}-E(x^{k})}{\sqrt{var(x^{k})}}$<br>这么做的优点是可以加快收敛的速度，但是缺点是假如该层的各个特征互不相关，简单的正则化操作可能会改变该层的特征表达。<br>为了确保神经网络里面可以进行恒等变换(identity transform)，我们需要对常规的正则化公式进行改变。由此我们也引入了Batch Normalization:$BN_{\gamma,\beta}$。 $y^{k}=\gamma^{k}\hat{x^{k}}$;$\beta^{k}$<br>其中每一个activation都会引入两个超参数$\gamma,\beta$。这两个参数也是神经网络需要学习的参数，分别起到的是scale和shift的作用。相当于在原来正则化的基础上，再进行了一次线性变化。在mini-batch的训练过程中，BN是规范化了每一层的输入，$z=g(BN(W,u))$  「g表示的非线性操作，例如：relu」，从而减少了Internal Covariate shift的干扰。</p>
<blockquote>
<p>the inputs to each layer are affected by the parameters of all preceding layers - so that small<br>changes to the network parameters amplify as the network becomes deeper …… Fixed distribution of inputs to a<br>sub-network would have a positive consequences for the layers outside the network, as well.  此外，BN的创新也在于<br>applied to the sub-networks and layers, incorporate the normalization in the network architecture as well。  </p>
</blockquote>
<p>Batch Normalization不仅允许不那么谨慎的初始化，还允许使用更高的learning rate。</p>
<h2 id="Rethinking-the-Inception-Architecture-for-Computer-vision"><a href="#Rethinking-the-Inception-Architecture-for-Computer-vision" class="headerlink" title="Rethinking the Inception Architecture for Computer vision"></a>Rethinking the Inception Architecture for Computer vision</h2><p>因为计算开销、参数量限制了把Inception部署到移动端和一些场景中，在abstract里，作者指出了对网络构架进行改进的思路。</p>
<blockquote>
<p>Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by <strong>factorized convolutions and aggressive regularization</strong>.</p>
</blockquote>
<h3 id="卷积核的因式分解"><a href="#卷积核的因式分解" class="headerlink" title="卷积核的因式分解"></a>卷积核的因式分解</h3><p>Paper里探索了几张将较大的卷积核分解为较小的卷积核的设置方式。例如：5x5的卷积核替换为两个3x3的卷积核；3x3的卷积核替换为1x3和3x1的卷积核。这样具有相同的receptive field的同时可以大大的减小计算开销。</p>
<p><img src="https://tse2.mm.bing.net/th?id=OIP.lEHUl5w_rweJSb-FpNdKeAHaFl&amp;pid=Api" alt="1">  </p>
<p><img src="http://davidstutz.de/wordpress/wp-content/uploads/2017/03/inception_arch_3.png" alt="2">  </p>
<p>需要注意的是「nxn的卷积被替换为1xn和nx1的卷积」的这种空间上分解为非对称卷积的做法在前面几层layer的效果不是很好，更适用于中等规格大小的feature map，（m的范围从12到20）。<br>此外我们还要思考🤔几个问题。<br>1.用小卷积核替换大卷积核，是否会带来信息损失(loss of expressiveness)? 不会，只要多次叠加的小卷积核和开始的大卷积核具有相同的receptive field。<br>2.如果我们的目标是对计算开销中的线性部分进行因式分解，那么为什么不直接在第一次保持线性激活（linear activation）？因为在实验中表明，非线性激活性能更好。</p>
<h3 id="Label-Smoothing-Regularization"><a href="#Label-Smoothing-Regularization" class="headerlink" title="Label Smoothing Regularization"></a>Label Smoothing Regularization</h3><p>因为大多数的数据集都存在错误的标签， 但是minimize the cost function on the wrong labels can be harmful。因此在Model Regularization中，可以通过在训练的过程中主动加入噪声作为penalty，这样的模型具有noise Robustness。Label Smoothing Regularization(LSR)是其中的一种regularization的方法。</p>
<blockquote>
<p>Here we propose a mechanism to regularize the classifier layer by estimating the marginalized effect of label-dropout during<br>training.   </p>
</blockquote>
<p>{<em>举一个University of Waterloo的WAVE LAB的 ME 780中lecture 3：Regularization for deep models的例子来帮助理解</em>：<br><em>ground-truth:</em>   y1_label=[1,0,0,……，0]<br><em>prediction:</em>   经过softmax classifier得到的softmax output:  y1_out=[0.87,0.001,0.04……,0.03]. }  </p>
<blockquote>
<p>maximum likelihood learning with softmax classifier and hard targets may actually never converge, the softmax can<br>never predict a probability of exactly 0 or 1, so it will continue to learn larger and larger weights, making more<br>extreme predictions.  </p>
</blockquote>
<p>假设x为training example，$p(k|x)$为x属于「label k」的概率，$q(k|x)$为x属于「ground-truth label」的概率。为了方便起见，忽略了p和q在example x上的相关性。</p>
<p>目标函数：「最小化交叉熵」。因为交叉熵衡量的是两个分布（p和q）的相似性，最小化目标函数是为了让预测的label概率分布$p(k|x)$（即例如上面的softmax的输出）和ground-truth label的概率分布$q(k|x)$尽可能的接近。「最小化交叉熵」也等价为「最大化似然函数」。但是我们需要对这个目标函数进行了改进。因为在单类情况下，单一的交叉熵导致样本属于某个类别的概率非常大，模型太过与自信自己的判断。这样会导致过拟合，此外还会降低模型的适应能力。为了避免模型过于自信，引入了一个独立于样本分布的变量u(k)，这相当于在ground-truth distribution中加入了噪声，组成一个新的分布。在实验中，使用的是均匀分布(uniform distribution)代替了u(k)</p>
<blockquote>
<p>we propose a mechanism for encouraging the model to be less confident. While this may not be desired if the goal is to maximize the log-likelihood of training labels, it does regularize the model and makes it more adaptable …… we refer to this<br>change in ground-truth label distribution as label-smoothing regularization, or LSR.  </p>
</blockquote>
<h2 id="Resnet"><a href="#Resnet" class="headerlink" title="Resnet"></a>Resnet</h2><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>一般来说，模型的深度加深，学习能力增强，但是不能简单的增加网络的深度，否则会出现随着网络深度增加，training error和test error变高的现象。这也说明网络结构变复杂时，optimization变得更加困难。<br>Kaiming He提出了深度残差学习（deep residual learning framework）,通过网络结构的创新来有效的解决了上述的梯度弥散现象(degradation)。</p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://camo.githubusercontent.com/6766d20252ed7afb1b3e17d92f38020da2328f5e/68747470733a2f2f696d672d626c6f672e6373646e2e6e65742f32303137303232303230313132383933383f77617465726d61726b2f322f746578742f6148523063446f764c324a736232637559334e6b626935755a58517664334e77596d453d2f666f6e742f3561364c354c32542f666f6e7473697a652f3430302f66696c6c2f49304a42516b46434d413d3d2f646973736f6c76652f37302f677261766974792f43656e746572" alt="1">  </p>
<p>从以下两点来分析网络结构的创新  </p>
<ul>
<li><strong><em>shortcut connection</em></strong><br>残差的网络结构是前向神经网络+shortcut。从上图可以看出在已有的网络结构中增加了一个branch，起到的是恒等映射（identity mapping）的作用，这样保证了一个深度模型的training error最起码保证shallow counterpart模型的training error是一致的，错误率不会高于浅层。此外，这种做法既不会增加额外的参数也不会增加额外的计算量。</li>
<li><strong><em>residual representations</em></strong><br>两种函数的表达效果是相同的，但是优化的难度是不同的。对残差进行拟合显然要更加容易。<br>残差函数  $F(x):=H(x)-x$。引入残差后的映射对输出的变化更加敏感，如H(5)=5.1,F(5)=0.1, 输出从5.1变化到5.2，增加的幅度为2%，但是残差是从0.1变化到0.2，增加幅度为100%。<em>残差的思想就是去除相同的主体部分，突出微小的变化</em>。  </li>
</ul>
<h3 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h3><p>$${y}=F({x},{W_i})+W_s{x}$$<br>其中$F({x},{W_i})$和$x$是需要学习的残差映射，在计算的时候需要保证维数一致，考虑到在两个feature map 中进行逐元素相加（element-wise addition)。当维数不一致的时候，通过引入线性映射W_s来匹配维度</p>
<p>最终取得的效果是：</p>
<blockquote>
<p>these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.</p>
</blockquote>
<h2 id="Inception-Resnet"><a href="#Inception-Resnet" class="headerlink" title="Inception-Resnet"></a>Inception-Resnet</h2><p>因为Residual connections在训练深度网络时，十分有优势，再加上inception网络的深度也比较深，所以作者尝试将两种方法结合起来。更确切的说是，将Inception中的filter concatenation中的一部分替换为residual connections的结构。结合了residual connection的inception模块如下所示：  </p>
<p><img src="https://camo.githubusercontent.com/8c540abfc8714b56027ee7545f56967263c1b74a/68747470733a2f2f706963312e7a68696d672e636f6d2f76322d34393234373065313638376531653534373135366565393033363462346636305f722e6a7067" alt="2">   </p>
<p>但是引入residual connection后，网络太深，稳定性不好，因此再次做了修改。<br><img src="https://camo.githubusercontent.com/ade4e987c83d1aa34417a65032bfa073112ebb39/68747470733a2f2f706963322e7a68696d672e636f6d2f38302f76322d64363535373237343964313535336639636366353062636136353961366666645f68642e6a7067" alt="3"><br>图中inception框可以用任意其他subnetwork替代，但是这次修改是在输出后引入了缩放系数(scale),再相加和激活。<br>文章提出了两个版本：Inception-ResNet v1和Inception-ResNet v2，相比原来，加快训练的收敛速度。在图像识别，视频检测等领域都作为了base-network。</p>
<blockquote>
<p>In the experimental section we demonstrate that it is not very difficult to train competitive very deep networks without utilizing residual connections. However the use of residual connections seems to <strong><em>improve the training speed greatly</em></strong>, which is alone a great argument for their use.  </p>
</blockquote>

      
    </div>
    
    
    

    

    

    
      <div>
        
            <div>
	
		<div style="text-align:center;color: #ccc;font-size:14px">----------------------
本文结束----------------------</div>
	
</div>
        
      </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/入门/" rel="tag"><i class-"fa="" fa-tag"=""></i>入门</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/16/loss/" rel="prev" title="FAIR视觉论文集锦">
                FAIR视觉论文集锦 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- Go to www.addthis.com/dashboard to customize your tools -->
<div class="addthis_inline_share_toolbox">
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=pubid=ra-5c392c3b907a4377" async="async"></script>
</div>

      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="Hang Yang">
            
              <p class="site-author-name" itemprop="name">Hang Yang</p>
              <p class="site-description motion-element" itemprop="description">在阳光灿烂的日子里开怀大笑</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atmol.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/hangcheers" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="csuyanghang@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#GoogleNet"><span class="nav-number">1.</span> <span class="nav-text">GoogleNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction-amp-Motivation"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction &amp; Motivation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inception-module"><span class="nav-number">1.2.</span> <span class="nav-text">Inception module</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GoogleNet’s-architecture"><span class="nav-number">1.3.</span> <span class="nav-text">GoogleNet’s architecture</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">2.</span> <span class="nav-text">Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Internal-covariate-shift"><span class="nav-number">2.1.</span> <span class="nav-text">Internal covariate shift</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normalization"><span class="nav-number">2.2.</span> <span class="nav-text">Normalization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Rethinking-the-Inception-Architecture-for-Computer-vision"><span class="nav-number">3.</span> <span class="nav-text">Rethinking the Inception Architecture for Computer vision</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积核的因式分解"><span class="nav-number">3.1.</span> <span class="nav-text">卷积核的因式分解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Label-Smoothing-Regularization"><span class="nav-number">3.2.</span> <span class="nav-text">Label Smoothing Regularization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Resnet"><span class="nav-number">4.</span> <span class="nav-text">Resnet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#introduction"><span class="nav-number">4.1.</span> <span class="nav-text">introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网络结构"><span class="nav-number">4.2.</span> <span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算公式"><span class="nav-number">4.3.</span> <span class="nav-text">计算公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inception-Resnet"><span class="nav-number">5.</span> <span class="nav-text">Inception-Resnet</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Yang</span>

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
</div>








        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>