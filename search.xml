<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pandas 之groupby 函数]]></title>
    <url>%2F2019%2F06%2F17%2Fdata_analyst_3%2F</url>
    <content type="text"><![CDATA[Pandas是Python里面专门用于数据分析的工具包。个人还蛮推荐这本e-book的Python for Data Analysis ～12345678import pandas as pdipl_data = &#123;'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings', 'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'], 'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2], 'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017], 'Points':[876,789,863,673,741,812,756,788,694,701,804,690]&#125;df = pd.DataFrame(ipl_data) 1print(df) Team Rank Year Points 0 Riders 1 2014 876 1 Riders 2 2015 789 2 Devils 2 2014 863 3 Devils 3 2015 673 4 Kings 3 2014 741 5 kings 4 2015 812 6 Kings 1 2016 756 7 Kings 1 2017 788 8 Riders 2 2016 694 9 Royals 4 2014 701 10 Royals 1 2015 804 11 Riders 2 2017 690 1print(df.groupby('Team')) &lt;pandas.core.groupby.groupby.DataFrameGroupBy object at 0x10be6c470&gt; 1print(df.groupby('Team').groups) # 查看分组情况 {&apos;Devils&apos;: Int64Index([2, 3], dtype=&apos;int64&apos;), &apos;Kings&apos;: Int64Index([4, 6, 7], dtype=&apos;int64&apos;), &apos;Riders&apos;: Int64Index([0, 1, 8, 11], dtype=&apos;int64&apos;), &apos;Royals&apos;: Int64Index([9, 10], dtype=&apos;int64&apos;), &apos;kings&apos;: Int64Index([5], dtype=&apos;int64&apos;)} 12345grouped = df.groupby('Team')# 迭代遍历分组for name_team,group in grouped: print(name_team) print(group) Devils Team Rank Year Points 2 Devils 2 2014 863 3 Devils 3 2015 673 Kings Team Rank Year Points 4 Kings 3 2014 741 6 Kings 1 2016 756 7 Kings 1 2017 788 Riders Team Rank Year Points 0 Riders 1 2014 876 1 Riders 2 2015 789 8 Riders 2 2016 694 11 Riders 2 2017 690 Royals Team Rank Year Points 9 Royals 4 2014 701 10 Royals 1 2015 804 kings Team Rank Year Points 5 kings 4 2015 812 1print(grouped.get_group('Kings')) Team Rank Year Points 4 Kings 3 2014 741 6 Kings 1 2016 756 7 Kings 1 2017 788 12import numpy as npprint(grouped['Points'].agg([np.mean, np.sum])) mean sum Team Devils 768.000000 1536 Kings 761.666667 2285 Riders 762.250000 3049 Royals 752.500000 1505 kings 812.000000 812 12# 查看每个分组的大小print(grouped.agg(np.size)) Rank Year Points Team Devils 2 2 2 Kings 3 3 3 Riders 4 4 4 Royals 2 2 2 kings 1 1 1 123# filter some datafilter = df.groupby('Team').filter(lambda x: len(x) &gt;= 3)print(filter) Team Rank Year Points 0 Riders 1 2014 876 1 Riders 2 2015 789 4 Kings 3 2014 741 6 Kings 1 2016 756 7 Kings 1 2017 788 8 Riders 2 2016 694 11 Riders 2 2017 690 12# conclude# df.groupby 主要用于分割对象，应用函数「聚合，转换，过滤」等]]></content>
      <categories>
        <category>磨刀不误砍柴工</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[堆栈stack & 队列queue(I)]]></title>
    <url>%2F2019%2F06%2F05%2Fbasis%20of%20Data%20Structure%2F</url>
    <content type="text"><![CDATA[Stack Stack ADT: A list with the restriction that insertion and deletion can be performed only from one end, called Top 举几个简单的例子来理解一下堆栈的property「 Last In First Out 」 食堂的餐盘，放在最上面的最先被拿走 以及 输入编辑器里面的Undo (撤销) 操作 我们来看一下push (x) &amp; pop ( ) 的伪代码和示意图，来加深对这两个操作的理解。 1234567# pushif Top = MaximumSize: return (&quot;Stack is full&quot;)else: Top = Top + 1 ArrayStack(Top) = new item 12345# pop if Top = 0: return (&quot;Stack is empty&quot;)else: Top = Top - 1 然后我们再来看一下堆栈的应用， 应用1: Infix Expression to Postfix Expression。「参考这个视频进行整理哒」。 也就是把a op b 变成 ab op 栗子🌰 Infix: A + B C - D E Postfix: ABC + DE - 栗子🌰 Infix: ((A + B)C - D) E Postfix: AB+C D - E 堆栈里面的操作符（operator）的push和pop是和其precedence（优先级）相关。通俗点讲就是先乘除后加减🐶 用python来实现一下这个东西。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Stack: def __init__(self): self.items = [] self.length = 0 def push(self, val): self.items.append(val) self.length += 1 def empty(self): return self.length == 0 def pop(self): if self.empty(): return None self.length -= 1 return self.items.pop() def peek(self): if self.empty(): return None return self.items[0] def __str__(self): return str(self.items)precedence = &#123;&apos;*&apos;:3,&apos;/&apos;:3,&apos;+&apos;:2,&apos;-&apos;:2,&apos;(&apos;:1&#125; # dict to store the operators # 数字越大，优先级越高def convert(expression): print(__convert(expression.split()))def __convert(tokens): stack = Stack() res = [] for token in tokens: if token.isalpha(): res.append(token) elif token == &apos;(&apos;: stack.push(token) elif token == &apos;)&apos;: while True: temp = stack.pop() if temp is None or temp == &apos;(&apos;: break elif not temp.isalpha(): res.append(temp) else: if not stack.empty(): temp = stack.peek() while not stack.empty() and precedence[temp] &gt;= precedence[token] and token.isidentifier(): res.append(stack.pop()) temp = stack.peek() stack.push(token) while not stack.empty(): res.append(stack.pop()) return resconvert(&quot;A * ( B + C ) + D&quot;)# [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;+&apos;, &apos;D&apos;, &apos;+&apos;, &apos;*&apos;] 应用2: Depth First Search (DFS) 。「参考这个网站 进行整理哒」。 DFS traverses a graph in depthward motion and uses a stack to remember to get the next vertex to start a search , when a dead end occurs in any iteration. Rule: Step 1: Visit the adjacent unvisited vertex. Mark it as visited, display it and push it in stack. Then we explore each adjacent vertex that is not included in the visited set. Step 2: If no adjacent vertex is found, pop up a vertex from the stack. Step 3: Repeat Step 1 &amp; Step 2 until the stack is empty. 然后我们再用python来实现一下dfs这个思想。 123456789101112131415161718# non-recursivedef dfs_iterative(graph, start): visited, stack = set(), [start] # set to keep track of visited vertices while stack: vertex = stack.pop() if vertex not in visited: visited.add(vertex) stack.extend(graph[vertex] - visited) return visited # recursive def dfs_recursive(graph, start, visited = None): if visited is None: visited = set() visited.add(start) for n in graph[start] - visited: dfs_recursive(graph, n, visited) return visited Queue Queue ADT: a queue is open at both its end.{个人觉得这句话特别重要，stack只有一边开的，所以后面进来的先出去。而queue就不一样了，两边都是开的，所以也才有First In First Out 这个property}. One end is always used to insert data ，也就是rear or tail 这一端进行enqueue这个操作。The other is used to remove data，也就是front or head 这一端进行dequeue这个操作。举个生活中的例子：比如在机场外面打的士的时候，先来的先走呀～🐶 Breath First Search algorithm traverses a graph in a breathward motion and uses a queue to remember to get the next vertex to start a search, when a dead end occurs in any iteration.]]></content>
      <categories>
        <category>磨刀不误砍柴工</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Sorting Algorithm（III)]]></title>
    <url>%2F2019%2F05%2F30%2Ftree%2F</url>
    <content type="text"><![CDATA[这次整理的笔记主要内容是heap sort，但在说heap sort之前，先说一下树的结构和heap的结构。 Tree Data Structure 数组、链表、堆栈、队列都是线性结构，而树🌲是非线性结构的。所以如果是一些hierarchical data，就用树这种数据结构来存储，比较合适。 一般来说，树的高度和时间开销是有关的，如果树的高度越小，常规operation的时间开销也就越小。我们来关注一下height: Height of the empty tree = -1 Height of tree with one node = 0 keep tree balance &lt;—— make it dense and minimize its height 1234567 root / \... home / \ ugrad course / / | \ ... cs101 cs112 cs113 像binary tree可以分为 fully binary tree / complete binary tree / balanced binary tree / binary search tree. 简单提及一下这些不同种类的binary tree的性质。Fully Binary Tree 每一个节点都有 0 / 2 个子节点; Complete Binary Tree 每一个节点都有 2 个子节点 {the nodes are as left as possible}; Balanced Binary Tree 的高度是 $log_2 n$ ; Binary Search Tree (BST) 的 left child 的值要小于root的值，right child 的值要大于root的值。BST 的search(x) / insert(x) / remove(x)的average-case running time都是O(logn), 因为每次比较的时候search space都是对半对半的减少的～😁 下面学习一下树的构建以及遍历。常见的depth first traversal的方式有三种：in-order traversal (左子树、根节点、右子树) ， pre-order traversal (根节点、左子树、右子树) ，post-order traversal (左子树、右子树、根节点) 123456789101112131415161718192021222324252627class Node: def __init__(self, key): self.left = None self.right = None self.val = key# tree traversaldef Inorder(root): if root: Inorder(root.left) print(root.val) Inorder(root.right)def Postorder(root): if root: Postorder(root.left) Postorder(root.right) print(root.val)def Preorder(root): if root: print(root.val) Preorder(root.left) Preorder(root.right) 如果结合post-order和in-order两个arr[ ] 来构造一棵树，思路是。post[ ]的最后一个节点为root, 在in[ ]中找到root的index，root的左边都为左子树，右边都为右子树。类似下面，然后分别在左子树和右子树里面不断recursively。 123 1 / \[4, 8, 2, 5] [6, 3, 7] 除了DFS 还有 BFS (Breadth First Traversal) / Level Order。BFS starts visiting nodes from root while DFS starts visiting nodes from leaves。下面👇这个图可以比较直观的帮助理解。如果是BFS的方式进行遍历的话，得到的就是12345 插一句题外话，BFS经常在graph用，看一个graph traversal pseudo code BFS(g, s) create a queue Q Q.enqueue (s) mark s as visited while (Q is not empty): v = Q.dequeue // dequeue the top element for all neighbors w of v in graph G if w is not visited Q.enqueue( w ) mark w as visited Heap A heap is a special tree-based data structure in which the tree is a complete binary tree.{性质上面👆有提到哈～} 此外heap 通常被称为 Priority queue。 Queue 允许的操作是先进先出，在队尾插入元素，在队头取出元素。Heap也是在heap的末端插入元素，弹出root，但heap中元素的排列是按照一定的优先顺序进行排列的。 heap通常可以分为两类Max-heap 和 Min-heap。这个是由heap的性质决定的，如果任一节点的值是其子树所有节点的最大值，就是max-heap。如果我们这时候extract max就是取出root，但这个时候为了保证heap的内部顺序，我们需要max-heapify。同理如果插入一个新节点的话，如果此时的父结点比自己小，就要交换新节点和父结点的位置。不断往上寻找，一直到所有节点都满足条件为止。 首先我们来看一下binary heap的索引，root的index是1，假设node的index是p，parent[node]的index是p//2，left_child的index是2p，right_child的index是2p+1 12345 parent / Node / \L_child R_child 然后我们来看一下max-heapify的pseudo code 12345678910111213Max-Heapify (A,i) l &lt;—— left-child index R &lt;—— right-child index if l &lt; heap_size[A] and A[l] &gt; A[i]: greatest &lt;-- l else: greatest &lt;-- i if r &lt; heap_size[A] and A[r] &gt; A[greatest]: greatest &lt;-- r if greatest != i swap( A[i], A[greatest] ) Max-Heapify(A, greatest) end if]]></content>
      <categories>
        <category>磨刀不误砍柴工</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Sorting Algorithm（II)]]></title>
    <url>%2F2019%2F05%2F29%2Fsorting%20algorithm%2F</url>
    <content type="text"><![CDATA[1. Divide and Conquer Algorithm通常解决问题分为三步：1. Divide 把大问题拆解为同样类型的小问题 2. Conquer recursivley解决这些小问题 3. Combine 把之前获得的子问题的解给结合起来。(所以通常a divide-and -conquer algorithm makes multiple recursive calls). 在sorting algorithm里面用到divide and conquer的代表方法有 merge sort 和 quick sort。这两种方法的 running time比前面提到过的Bubble sort，Insertion sort，Selection sort要少。 Merge sort : Worst-Case running time $O(nlgn)$ Average-Case running time $O(nlgn)$ Best-Case running time $O(nlgn)$ Quick sort: Worst-Case running time $O(n^2)$ Average-Case running time $O(nlgn)$ Best-Case running time $O(nlgn )$ 2. Merge Sort 如果arr[ ] 中只有一个元素，那就是已经sorted，return Divide takes a recursive approach to divide the problem until no sub-problem. In merge sort, we divide the list recursively into two halves until the sub-array has only 1 element. n ——n/2 ——n/4 —— n/8 Divide only takes $O(1)$ time. merge the smaller lists into new list in sorted order. merge的方式是conquer the sublists together 2 at a time to produce new sorted sublists until all elements have been fully merged into a single sorted array. The merge function runs in $O(n)$ when merging n elements and the merge sort runs in $O(nlg n )$ time. 首先我们来看一下$O(nlg n)$ 是怎么得到的～ 想象一下有一个树🌲从上到下的过程中不断的二分二分二分，我们开始的第一个level有n个元素，最后一个level 每一个都只有一个元素。不过每一个level的元素个数总和都是n。merge sort的总时间等于对merge每一层的时间求和 (sum)，假设每一层Merge sort所需要的时间是cn,(c相当于一个constant) ，所以如果我们有L层的话，我们最终可以得到 L*cn 。根据binary tree的性质我们知道 $ L = log_2(n) + 1 $ 所以merge sort的时间是$cn(log_2n)$ ,所以我们得到了$O(nlg n)$然后举个例子来帮助理解Merge Sort. 我们现在有一个未排序的数组 arr[ ] = [14, 33, 27, 10, 35, 19] 首先我们要divide 这个数组。 Left_arr[ ] = [14 33 27] Right_arr[ ] = [10 35 19] Left_sub_arr1 = [14] Left_sub_arr2 = [33 27] Right_sub_arr1 = [10] Right_sub_arr2 = [35 19] , 再一次进行divide，最后我们得到的 sub_array = 14\ 33\ 27\10\35\19 现在我们要进行merge. Left_arr_merge1 = [14] Left_arr_merge2 = [27,33], Right_arr_merge1 = [10] Right_arr_merge2 = [19 35] Left_arr = [14,27,33] Right = [10 19 35] Sorted_arr = [10, 14, 19, 27, 33, 35] 附上python实现的代码 🤭 12345678910111213141516171819202122232425262728293031def mergeSort(arr): print("original array is", arr) if len(arr) &gt; 1: mid = len(arr) // 2 lefthalf = arr[:mid] righthalf = arr[mid:] mergeSort(lefthalf) mergeSort(righthalf) i = 0 j = 0 k = 0 while i &lt; len(lefthalf) and j &lt; len(righthalf): if lefthalf[i] &lt; righthalf[j]: arr[k] = lefthalf[i] i = i + 1 else: arr[k] = righthalf[j] j = j + 1 k = k + 1 while i &lt; len(lefthalf): arr[k] = lefthalf[i] i = i + 1 k = k + 1 while j &lt; len(righthalf): arr[k] = righthalf[j] j = j + 1 k = k + 1 print("merging", arr) Quick Sort首先要涉及到Pivot value的选择，因为Pivot value 和partition 有关，partition 又是 quick sort算法实现过程的核心。 Always pick first element as pivot {只是为了简单化} Put pivot at its correct position in sorted array and put all smaller elements (smaller than pivot) before pivot, and put all greater elements (greater than pivot) after pivot. {Done in the linear time} 然后我们来看一下quick sort的伪代码 1234567891011 quickSort( arr[ ], start_idx, end_idx)&#123; if ( start_idx &lt; end_idx) pivot = partition(arr, start_idx, end_idx) quickSort(arr, start_idx, pivot - 1) qucikSort(arr, pivot + 1, high) &#125; 我们再来进行一下time complexity的分析～ its worst-case running time is as bad as selection sort’s and insertion sort’s: $O(n^2 )$ 。参考可汗学院的algorithm教程，我们可以知道worst-case就是the most unbalanced partitions 。如下图所示。 同理，best case occurs when the partitions are evenly balanced as possible. 如下图所示。 但是quick sort的average-case running time 是和merge-sort一样的，都是$O(nlg n)$. In practice, quicksorts outperforms merge sort. Difference between Merge Sort and Quick SortIn Merge Sort, divide 几乎没起到什么作用，主要起作用的部分是merge部分。「mergesort」就是在merge部分✅～ In Quick Sort, merge 几乎没起到什么作用，主要起作用的部分是divide部分。「和pivot比较大小，从而分割」]]></content>
      <categories>
        <category>磨刀不误砍柴工</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Sorting Algorithm（I)]]></title>
    <url>%2F2019%2F05%2F29%2FSorting_Algorithm%2F</url>
    <content type="text"><![CDATA[因为之前没有系统学data structure and algorithm，暑假亡羊补牢一下（🤦‍♀️ 把这些常规的都用python给实现一遍。先从sorting开始，sorting可以分为internal sorting和external sorting。external 指的是 all data cannot be placed in-memory at a time, 常见的external sorting algorithm有merge-sort。不过any sorting algorithm can be used to sort the data that has been loaded into memory。 下面举的例子都假设array是按照升序进行排列的。😆此外 Running time is an important thing to consider when selecting a sorting algorithm since efficiency is often though of in terms of speed。所以都尝试分析一下best time complexity和worst time complexity。 Bubble Sort (internal sorting) Bubble Sort 是本科接触的第一个☝️排序算法， Bubble sort steps through the list and compares adjacent pairs of elements. The elements are swapped if there are in the wrong order. 但是也正是因为冒泡法从头到尾的遍历过程中需要两两相比较，如果比较的很顺利，也就是整个array其实已经是排序好了的，这时候的time complexity 就是$o(n)$，如果整个array是乱七八糟的，这时候也就是worst time complexity $o(n^2)$ 12345678def bubbleSort(arr): n = len(arr) # traverse through all array elements for i in range(n): for j in range(0, n - i - 1): # traverse the array from 0 to n-i-1 if arr[j] &gt; arr[j + 1]: arr[j], arr[j + 1] = arr[j + 1], arr[j] Insertion Sort (internal sort) On each loop iteration, insertion sort removes one element from the array. It then finds the location where that element belongs within another sorted array and inserts it there. 我们先来看一下插入排序的pesudo code。 Loop from i = 1 to n-1 ​ pick element arr[i], and insert it into the sorted sequence arr[0...i-1] 然后再举个例子来理解， arr[] = 11 13 10 20 1st loop: 11 13 10 20 （since 11&lt; 13) 2nd loop: 10 11 13 20 (since 10 &lt; 11) 3rd loop: 10 11 13 20 (since 20 &gt; 13) 现在我们来看一下time complexity。好的情况就是没有swap的过程，和上面bubble一样，整个array其实已经是排序好了的，所以best time complexity $o(n)$ 。坏的情况就是我们需要的是升序的，但一开始的array是降序排列的, # opertaion = n(n-1) 所以worst time complexity $o(n^2)​$ 12345678def insertion_sort(arr): for i in range(1, len(arr)): key = arr[i] j = i - 1 while j &gt;= 0 and key &lt; arr[j]: arr[j], arr[j + 1] = arr[j + 1], arr[j] j = j - 1 return arr Selection Sort (internal sort) The algorithm maintains two subarrays in a given array. 1)The subarray which is already sorted 2) Remaining subarray which is unsorted。也就是我们把一个数组分为已经排列整齐的 和 乱七八糟的两类。在乱七八糟的那一部分找到一个最小的元素，把它放到已经排列好的那一部分的数组中正确的位置中去。 用上面同样的例子来理解一下 arr[] = 11 13 10 20 10 // find the minimum element in arr[0…3] put at the beginning 10// sorted 11 13 20// unsorted 11 // find the minimum element in arr[1…3] put at the second place 10 11 //sorted 13 20 // unsorted 13 // find the minimum element in arr[2..3] put at the third place then we get 10 11 13 20 12345678def selection_sort(arr): for i in range(len(arr)): minimal = i for j in range(i + 1, len(arr)): if arr[j] &lt; arr[minimal]: minimal = j arr[minimal], arr[i] = arr[i], arr[minimal] return arr]]></content>
      <categories>
        <category>磨刀不误砍柴工</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[plotting]]></title>
    <url>%2F2019%2F01%2F13%2Fplotting%2F</url>
    <content type="text"><![CDATA[Hello, 2019年的第一篇博客。依旧是介绍一下我最近使用的几个很好用的工具。 百度开源的pyecharts。 之前「机器之心」认识的小伙伴weng jj推荐给我的。引用pyecharts官网文档的介绍，pyecharts相当于python版本的Echarts，是百度开源的一个数据可视化JS库。可以画饼状图、玫瑰图、折线图等多种类型的图。 下面举一个我做的一个很简单的例子。数据来源是kaggle上分析法国地区不平等的情况，经过了数据清理之后（此处省略，因为很简单0-o），我们得到了表格。此时我们想用表中的数据给可视化出来，以便更加清楚的看到男性和女性以及不同的工作种类的人在每小时拿到的净收入的差距参考pyechart文档，我们可以得到下面的图。在全法国人均总工资最高的前十个地区，不同性别所拿到的工资的差异。 在全法国从事不同职业的人之间的工资差异。 从柱状图和上面的玫瑰图，我们就可以很轻而易举的看到差距啦（😁），而且pyecharts画出来的📈，比python带的matplotlib工具包画出的图表，视觉效果要好很多。]]></content>
      <categories>
        <category>磨刀不误砍柴工</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTex Installation on OSX]]></title>
    <url>%2F2018%2F12%2F19%2Flatex-installation-on-OSX%2F</url>
    <content type="text"><![CDATA[Latex在论文写作中用的比较多，比word好用不少。Latex最大的优势就是复杂公式的编辑与排版非常漂亮。今天记录一下在mac OSX环境下如何舒服的敲Latex。解决方案：Sublime Text + MacTex + Skim MacTeX是TeXLive的Mac版，在http://www.tug.org/mactex/中下载MacTex.pkg。 安装完成后，会出现如下界面。 Sublime Text 是编辑器，首先按「command + shift + P」来打开命令行，然后输入命令「Install Package」,按下Enter回车键。在输入完成后，再输入“LaTex Tools”并安装。我们可以在安装package的过程中，可以打开console。Console一般在ST3的底部，用来查看安装的进程。 . Skim是在OSX下面比较轻便的一个PDF阅读器。安装完阅读阅读器，需要「Sync」一下Sublime Text和Skim，如果下载的是Sublime Text3就选Sublime Text，如果是Sublime Text2就选择Sublime Text2。 PS，LaTex Templates上提供了很多简历模版，可以借鉴参考。]]></content>
      <categories>
        <category>磨刀不误砍柴工</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FAIR视觉论文集锦]]></title>
    <url>%2F2018%2F12%2F16%2Floss%2F</url>
    <content type="text"><![CDATA[FAIR在目标检测、实力分割等领域都做了创新。本篇博客从R-CNN到FPN到RetinaNet到Mask-RCNN做了一点归纳总结。 Faster-RCNN首先来提Faster-RCNN 网络结构，是因为Mask-RCNN是在其基础上改进网络结果「更具体点并列加一个mask branch」而得到的来实现segmentation。Faster-RCNN在object detection中相当于baseline system，也是benchmark。主要包括了对目标物体的分类（classification），以及用候选框（bounding box）来对图片中的位置进行定位。在此之前也已有了Fast-RCNN之类的目标检测算法了。文章「Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks」的创新在于解决了Region Proposal生成开销问题。当生成的候选框过多时，processing speed会受到影响，从而没法很好的实现real-time object detection。 we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals 在Faster-RCNN中使用RPN来进行候选框的确定，即「Region proposal Network找出物体可能存在的所有位置」，在这一个过程中找全，没有漏检很重要，不然后面的分类也没法分了。即Recall的值要高，「Recall=正确识别出来的object/数据库里含有的object，当recall=100%时，表示没有漏检」。RPN网络是一种全连接网络（FCN在下文有提到哈哈） RPN预测了object bounds and objectness scores at each position，这给Fast-RCNN起到类似指哪打哪的作用了。此外，这里也是文章的另一个创新点，通过「sharing the convolutional features」实现了RPN和Fast-RCNN融合到一个网络中去了。在这里我们来理解一下「sharing」，RPN从feature map 上选择出了一系列的bounding box，然后Fast-RCNN再次利用了feature map，并用ROI pooling（主要包括三步：1. 把 region proposal 分为n等分，n=the dimension of the output 2. 找到每个section最大的值 3.把每个最大的提取出来作为output buffer）来对每个candidate box进行classification 和 bounding box regression，也在一定程度上节省了计算开销，加速了训练过程。 using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the unified network where to look Feature Pyramid Network背景：ROI映射到某个feature map是将底层的坐标直接除以stride，显然对于小目标（size比较小）物体来说，到后面的卷积池化时，实际的语义信息就丢失了很多了。在CNN中，我们需要考虑不同种类的invariance来做识别，scale invariance很难被CNN考虑到。一般通常的做法有两种Image Pyramid和Feature Pyramid。其中Feature Pyramid的代表有SPPNet和FPN。 FPN结构：FPN是基于特征提取的网络，可以是ResNet也可以是DenseNet。在Mask-RCNN中就将ResNet和FPN相结合，作为base-network。常见的命名方式：主干网络-层数-FPN，如ResNet-101-FPN。在深度学习框架下，选取一个预训练模型就可以实现FPN。FPN设计的金字塔结构包括了bottom-up &amp; top-down &amp; lateral connections三种结构。bottom-up是主干CNN沿前向传输（feed-foward/inference)。top-down是上采样（upsampling) 。lateral connection通常使用 1x1的卷积，融合不同层的语义信息的同时降低维度，和upsampling叠加后得到不同分辨率的特征图，从而达到多尺度anchor的效果。 focal loss for dense object detection背景：目标检测中 根据有标签的数据划分 positive / negative training examples(其中通过 bounding box 的IOU来确定正负样本，IOU超过一定的阈值则为正样本)一般负样本会多于正样本.但是为了训练出来的模型不偏向于negative，需要保证样本数目的balance.Focal loss for dense object detection解决的就是 foreground-background即前景和背景的数据的imbalance。 创新：本文是通过「loss」改变为「focal loss」而不是「architecture」创新来speed/accuracy/complexity的trade-off，在这里指的一提的是主干网络「RetinaNet」。 Fully Convolutional NetworksFully Convolutional Networks for semantic segmentation 一开始说的： combines semantic information from a 「deep , coarse」 layer with appearance information from a 「shallow, fine」 layers 那为什么deep和coarse连在一起，shallow和fine来接在一起，不是越deep的层，越有表达力么？ 全连接网络和CNN之间的区别：经典的CNN是将卷积层产生的feature map使用全连接层映射为固定长度的特征向量，最后输出的是概率。FCN将全连接层都变化为卷积层，「E.X.: 将4096 变成1x1x4096」是针对语义分割训练的一个end-to-end, pixel的网络，最后输出的是heatmap热力图。 FCN网络结构创新点：FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map做上采样upsampling，使其恢复到输入图像的相同尺寸，从而对每一个像素都产生一个预测，同时保留原始输入图像的空间信息。但是这样得到的结果比较coarser, 一些细节不能恢复。因此，作者采用了skip architecture来优化上采样，即将不同池化层的结果进行上采样，然后结合这些结果来优化输出。「E.X 第五层的输出32倍放大反卷积到原图大小时比较粗糙，因此作者将第四层输出16倍放大，第3层输出8倍放大，可以从原论文中插图看到越低池化层，越精细」因此我们也就可以理解了上文的问题。 FCN在mask-RCNN中的应用：在the mask branch中，FCN被用在每个ROI中进行pixel-to-pixel的分割，这也是mask-RCNN超越了Faster-RCNN的地方。作者在文章里是这么说的： Our method, called Mask-RCNN，extends Faster-RCNN by adding a branch for predicting segmentation masks on each Region of Interest,in parallel with the existing branch for classification and bounding box regression. Mask-RCNNMask-RCNN论文地址 Mask-RCNN实现的任务要更「难」，因为不再是object detection 而是要达到instance segmentation，细化到区分类别中的不同实例。通俗点说，像素分类的话可以用不同的颜色来区别不同的实例，但是实例分割的时候即使是同一种类的物体，比如都是猫猫，也要区别出橘猫和加菲猫。像FCN中也可以用在实例分割的情景中，但它们的做法是，对每个像素进行multi-class categorization。作者提出的方法在实例分割中是更有优势的。 Instead, our method is based on parallel prediction of masks and class labels, which is simpler and more flexible.In contrast to the segmentation-first level of these methods, Mask R-CNN is based on an instance first strategy. 在上面介绍faster-RCNN时，已经提到了Mask-RCNN增加了分支，来预测物体对应的掩膜(object mask). 在阅读Mask-RCNN的时候，遇到一个问题「如何来理解pixel-to-pixel alignment 」 we propose a simple, quantiazation-free layer, called ROIAlign， that preserves exact spatial locations. faster-RCNN的ROI Pooling,ROI Pooling 存在两次量化（quantize）过程：第一次是将候选框的边界（通常是浮点数）量化成了整数点坐标，第二次是将量化后的边界区域平均分割成kxk个单元bin时，对每一个单元进行了量化。也可以理解为「粗暴的四舍五入」，使用了邻近插值法，从而选择离目标最近的点。但是这么做会带来一定的偏差，从而影响到分割的精确。也是文章中提到的misalignment。「放大后的图有马赛克，而缩小的图有失真」为了克服这一弊端，作者就取消了量化的过程了，使用双线性插值的方法。Mask-RCNN 用的方法是ROIAlign。 we use bilinear interpolation to compute the exact values of the input features at four regularly sampled locations in each ROI bin, and aggregate the result. 参考wiki双线性插值指的是对x，y方向各进行一次插值方法。在原图src(source)和目标图dst(destination)上进行图像的缩放。 充分利用src中四个真实的像素值来共同决定目标图中的一个像素值，缩放后的图像质量更高。假设src中四个点的坐标分别是（0，0）（0，1）（1，0）（1，1），公式如下所示： ROIAlign的做法带来的好处是： 1.it improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics. 2.we found it essential to decouple mask and class prediction: we predict the binary mask for each class independently.]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inception]]></title>
    <url>%2F2018%2F12%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Inception系列有四篇重要的paper，分别是：Going Deeper with Convolutions、Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shif、Rethinking the Inception Architecture for Computer Vision、Inception-v4在此，依次阅读并做笔记。 GoogleNetIntroduction &amp; MotivationGoing Deeper with Convolutions 首次提出了「Inception」模块作为网络构架，该网络构架也是后续作为classification和detection的base network的重要组成部分。 we introduce a new level of organization in the form of the “Inception module” and also in a more direct sense of increasednetwork depth. 网络的size主要从两方面进行考虑：depth - the number of levels - of the network 和 width - the number of units at each level。「加深网络depth」、「调节超参数」可以在recognition和object detection取得更好的效果。但是网络的size过大，会直接影响运行的性能。就像一个人过胖，会直接影响身体健康。当网络的size过大的时候，参数#paramters过多，消耗的计算资源就越多，此外，特别是在labeled examples很有限的情况下，更容易出现overfitting。一般是采用「dropout」或者「regularization」，并且「调整超参数」和「设置学习率」去防止训练过程中过拟合现象的出现。 For larger datasets such as Imagenet, deeper architectures are used to get better results and dropout is used to preventoverfitting …… Since in practice the computational budget is always finite, an efficient distribution of computing resources is preferred to an indiscriminate increase of size。 Inception module上面的方法挺好的，但也挺蛮烦的，所以作者试图结合「数据结构、网络结构」来考虑，如何设计一个创新性的architecture，来更好的利用计算资源以及稍微放心、大胆的设置参数一些? 首先，开个小分支，介绍一下「稀疏结构」的理论基础：Hebbian原理。 作者从neuroscience的角度得到了启发，提出了网络结构的创新：。该原理指出：各个神经元是组合效应，通过神经突触进行信息的传递，大脑皮层接收信息。此外，神经反射活动的持续与重复会导致神经元连接稳定性的持久提升，当两个神经元细胞A和B距离很近，并且A参与了对B重复、持续的兴奋，那么某些代谢变化会导致A将作为能使B兴奋的细胞。 neurons that fire together, wire together.将Fully Connected变为稀疏连接（sparse connection）的时候，可以在增加网络深度和宽度的同时减少参数个数，但是大部分的硬件是针对密集矩阵计算优化的，稀疏矩阵虽然数据量变少，但计算所消耗的时间很难减少。GoogleNet希望做的就是既保证网络结构的稀疏性、又利用密集矩阵的高计算性能。 GoogleNet的核心是Inception module，而Inception相当于一个Convolutional building block，也是一个局部稀疏最优解的网络构架，然后我们在空间上做堆叠。下面我们结合论文的插图来仔细分析一下Inception module。图a是原始的Inception module，图b是借鉴了NIN（Network In Network）引入1x1的卷积操作，改进后的Inception module。 Our network will be built from convolutional building blocks.All we need is to find the optimal local construction and to repeat it spatially. 输入有四个分支，使用多个尺度（1x1或3x3或5x5）的卷积和池化进行特征提取「相当于将稀疏矩阵分解为密集矩阵」，每一尺度提取的特征是均匀分布的，但是经过「filter concatenation」这步操作后，输出的特征不再是均匀分布的，相关性强的特征会被加强，而相关性弱的特征会被弱化。这个相关性高的节点应该被连接在一起的结论，即是从神经网络的角度对Hebbian原理有效性的证明「filter concatenation」，这一步其实相当于沿着深度方向（或者说在depth这个维度）进行拼接， stack up the first volume to the second volume to make the dimensions match up …… Output a single output vector forming the input ofnext stage。 结合Udacity视频和code来加深一下对「filter concatenation」的理解 concatenated_tensor = tf.concat(3,[branch1, branch2, branch3, branch 4]) E.g:{General}：输入 28x28x192 volume ，并列经过 1x1卷积操作、3x3卷积操作、5x5卷积操作、max-pool，分别得到28x28x64、28x28x128、28x28x32、28x28x32 volume, 将并列的volume沿着深度方向进行拼接，输出 28x28x256 volume。 Feifei-Li的cs231n的课件里是描述CNN的：every layer of a ConvNet transforms one volume of activations to another through a differentiable function.We use three main types of layers to build ConvNet architectures:Convolutional Layer, Pooling Layer,and Fully-Connected Layer. Conv layer will compute the output of neurons that are connected to local regions in the input,each computing a dot product between their weights and a small region they are connected to the input volume.Pool layer will perform a downsampling operation along the spatial dimensions(width,height)FC layer will compute the class score,resulting in volume of size「1x1x#class」。 {Specific}：5x5的卷积操作得到了28x28x32的block。filter size =5x5x192，5 pixels width and height, 192 pixels depth（filter的深度需要和前一feature map的深度保持一致。） 设input volume width = W, the width of receptive field = F_w, zero padding on the border = P, stride = S那么output volume width = (W-F+2P)/S+1。同理也可以得到output volume height。此外, input volume depth = D1此外，被filter覆盖的图像区域称为receptive field，具体操作是：slide each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position，即filter中的值和原始图像中receptive field中的像素值进行点积运算，产生activation map或feature map。图像一般都是局部相关的，第n+1层的每个神经元和第n层的receptive field中的神经元连接，而不需要和第n层的所有神经元连接，ConvNet具有local connectivity(局部连接) 的性质。当filter的receptive field越大，filter能够处理的原始输入内容的范围就越大。随着经过更多的卷积层，得到的激活映射也就具有更为复杂的特征。 设 number of filters = K, 也是output volume depth的值。当filter的数目越多，spatial dimensions就会保留的越好。CNN具有local connection和parameter sharing的特点。每个filter的权重的个数 = F_w x F_h x D1, 总的权重个数= F_w x F_h x D1 x K 我们再分析一下compution cost cs231n 指出： the largest bottleneck to be aware of when constructing the ConvNet is the memory bottle neck.we need to keep track of the intermediate volume size, the paramter size and the memory.Reference:cs231n 现在我们来分析一下，上面的图b相比图a的优势在哪里🧐。1x1的卷积是作为瓶颈层的作用，用很小的计算量可以增加一层特征变换和非线性变换。此外，一般涉及到改变通道数，都会使用1x1卷积操作，例如残差连接和Dense连接 the bottleneck is usually the smallest part of something我们来计算一下图a中5x5的卷积操作得到了28x28x32的block的时候，所需要的multiples的次数。以及图b中先使用1x1的卷积操作先得到28x28x16，再使用5x5的卷积操作得到了28x28x32的blcok的时候，所需要的multiples的次数。 1.图a (28x28x32) x (5x5x192) = 120million 「一个output volume所需要的乘积次数 x the number of output values」 2.图b （28x28x16) x (1x1x192) + (28x28x32) x (5x5x16) = 12.4 million 从上面👆两个对比可以知道1x1的卷积操作大大的减少了计算量。 GoogleNet’s architecture首先，为了有一个初步的印象，先截取了GoogleNet的一部分，我们可以注意到这里有一个「softmax」的分支，整个结构中有两个「softmax」，它相当于辅助分类器，结合code我们可以知道该操作是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中起到的是梯度前向传输的作用。论文里是这么交代的： By adding auxiliary classifiers connected to these intermediate layers, we would expect to encourage discrimination in the lower stages in the classifier, increase the gradient signal that gets propagated back, and provide additional regularization. 其次，为了对GoogleNet的组成有一个概念，引用了论文中的表格：上面讨论时，已经说过GoogleNet是模块化的，堆叠了多个Inception Module，靠后的Inception Module能够抽取更高阶的抽象的特征。 Batch NormalizationInternal covariate shift“Internal”指的是神经网络的隐含层，”Covariate”指的是输入的权重参数化，“Internal Covariate Shift”指的是在训练的过程中，输入的概率分布不固定，网络的参数在不断的变化，神经网络的隐含层也要不断的去「适应」新的分布。这个现象会让模型更加难训练，我们也需要更加谨慎的初始化模型参数和学习率。因此作者引入了Normalization 来解决这个问题。 Normalization通过规范化的手段，将每层神经网络任意神经元这个输入值的分布“强行拉回”到均值为0，方差为1的分布中.常规的正则化公式为：$\hat{x}^{k}=\frac{x^{k}-E(x^{k})}{\sqrt{var(x^{k})}}$这么做的优点是可以加快收敛的速度，但是缺点是假如该层的各个特征互不相关，简单的正则化操作可能会改变该层的特征表达。为了确保神经网络里面可以进行恒等变换(identity transform)，我们需要对常规的正则化公式进行改变。由此我们也引入了Batch Normalization:$BN_{\gamma,\beta}$。 $y^{k}=\gamma^{k}\hat{x^{k}}$;$\beta^{k}$其中每一个activation都会引入两个超参数$\gamma,\beta$。这两个参数也是神经网络需要学习的参数，分别起到的是scale和shift的作用。相当于在原来正则化的基础上，再进行了一次线性变化。在mini-batch的训练过程中，BN是规范化了每一层的输入，$z=g(BN(W,u))$ 「g表示的非线性操作，例如：relu」，从而减少了Internal Covariate shift的干扰。 the inputs to each layer are affected by the parameters of all preceding layers - so that smallchanges to the network parameters amplify as the network becomes deeper …… Fixed distribution of inputs to asub-network would have a positive consequences for the layers outside the network, as well. 此外，BN的创新也在于applied to the sub-networks and layers, incorporate the normalization in the network architecture as well。 Batch Normalization不仅允许不那么谨慎的初始化，还允许使用更高的learning rate。 Rethinking the Inception Architecture for Computer vision因为计算开销、参数量限制了把Inception部署到移动端和一些场景中，在abstract里，作者指出了对网络构架进行改进的思路。 Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by factorized convolutions and aggressive regularization. 卷积核的因式分解Paper里探索了几张将较大的卷积核分解为较小的卷积核的设置方式。例如：5x5的卷积核替换为两个3x3的卷积核；3x3的卷积核替换为1x3和3x1的卷积核。这样具有相同的receptive field的同时可以大大的减小计算开销。 需要注意的是「nxn的卷积被替换为1xn和nx1的卷积」的这种空间上分解为非对称卷积的做法在前面几层layer的效果不是很好，更适用于中等规格大小的feature map，（m的范围从12到20）。此外我们还要思考🤔几个问题。1.用小卷积核替换大卷积核，是否会带来信息损失(loss of expressiveness)? 不会，只要多次叠加的小卷积核和开始的大卷积核具有相同的receptive field。2.如果我们的目标是对计算开销中的线性部分进行因式分解，那么为什么不直接在第一次保持线性激活（linear activation）？因为在实验中表明，非线性激活性能更好。 Label Smoothing Regularization因为大多数的数据集都存在错误的标签， 但是minimize the cost function on the wrong labels can be harmful。因此在Model Regularization中，可以通过在训练的过程中主动加入噪声作为penalty，这样的模型具有noise Robustness。Label Smoothing Regularization(LSR)是其中的一种regularization的方法。 Here we propose a mechanism to regularize the classifier layer by estimating the marginalized effect of label-dropout duringtraining. {举一个University of Waterloo的WAVE LAB的 ME 780中lecture 3：Regularization for deep models的例子来帮助理解：ground-truth: y1_label=[1,0,0,……，0]prediction: 经过softmax classifier得到的softmax output: y1_out=[0.87,0.001,0.04……,0.03]. } maximum likelihood learning with softmax classifier and hard targets may actually never converge, the softmax cannever predict a probability of exactly 0 or 1, so it will continue to learn larger and larger weights, making moreextreme predictions. 假设x为training example，$p(k|x)$为x属于「label k」的概率，$q(k|x)$为x属于「ground-truth label」的概率。为了方便起见，忽略了p和q在example x上的相关性。 目标函数：「最小化交叉熵」。因为交叉熵衡量的是两个分布（p和q）的相似性，最小化目标函数是为了让预测的label概率分布$p(k|x)$（即例如上面的softmax的输出）和ground-truth label的概率分布$q(k|x)$尽可能的接近。「最小化交叉熵」也等价为「最大化似然函数」。但是我们需要对这个目标函数进行了改进。因为在单类情况下，单一的交叉熵导致样本属于某个类别的概率非常大，模型太过与自信自己的判断。这样会导致过拟合，此外还会降低模型的适应能力。为了避免模型过于自信，引入了一个独立于样本分布的变量u(k)，这相当于在ground-truth distribution中加入了噪声，组成一个新的分布。在实验中，使用的是均匀分布(uniform distribution)代替了u(k) we propose a mechanism for encouraging the model to be less confident. While this may not be desired if the goal is to maximize the log-likelihood of training labels, it does regularize the model and makes it more adaptable …… we refer to thischange in ground-truth label distribution as label-smoothing regularization, or LSR. Resnetintroduction一般来说，模型的深度加深，学习能力增强，但是不能简单的增加网络的深度，否则会出现随着网络深度增加，training error和test error变高的现象。这也说明网络结构变复杂时，optimization变得更加困难。Kaiming He提出了深度残差学习（deep residual learning framework）,通过网络结构的创新来有效的解决了上述的梯度弥散现象(degradation)。 网络结构 从以下两点来分析网络结构的创新 shortcut connection残差的网络结构是前向神经网络+shortcut。从上图可以看出在已有的网络结构中增加了一个branch，起到的是恒等映射（identity mapping）的作用，这样保证了一个深度模型的training error最起码保证shallow counterpart模型的training error是一致的，错误率不会高于浅层。此外，这种做法既不会增加额外的参数也不会增加额外的计算量。 residual representations两种函数的表达效果是相同的，但是优化的难度是不同的。对残差进行拟合显然要更加容易。残差函数 $F(x):=H(x)-x$。引入残差后的映射对输出的变化更加敏感，如H(5)=5.1,F(5)=0.1, 输出从5.1变化到5.2，增加的幅度为2%，但是残差是从0.1变化到0.2，增加幅度为100%。残差的思想就是去除相同的主体部分，突出微小的变化。 计算公式$${y}=F({x},{W_i})+W_s{x}$$其中$F({x},{W_i})$和$x$是需要学习的残差映射，在计算的时候需要保证维数一致，考虑到在两个feature map 中进行逐元素相加（element-wise addition)。当维数不一致的时候，通过引入线性映射W_s来匹配维度 最终取得的效果是： these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. Inception-Resnet因为Residual connections在训练深度网络时，十分有优势，再加上inception网络的深度也比较深，所以作者尝试将两种方法结合起来。更确切的说是，将Inception中的filter concatenation中的一部分替换为residual connections的结构。结合了residual connection的inception模块如下所示： 但是引入residual connection后，网络太深，稳定性不好，因此再次做了修改。图中inception框可以用任意其他subnetwork替代，但是这次修改是在输出后引入了缩放系数(scale),再相加和激活。文章提出了两个版本：Inception-ResNet v1和Inception-ResNet v2，相比原来，加快训练的收敛速度。在图像识别，视频检测等领域都作为了base-network。 In the experimental section we demonstrate that it is not very difficult to train competitive very deep networks without utilizing residual connections. However the use of residual connections seems to improve the training speed greatly, which is alone a great argument for their use.]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>入门</tag>
      </tags>
  </entry>
</search>
