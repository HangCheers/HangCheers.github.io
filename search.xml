<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pandas ä¹‹groupby å‡½æ•°]]></title>
    <url>%2F2019%2F06%2F17%2Fdata_analyst_3%2F</url>
    <content type="text"><![CDATA[Pandasæ˜¯Pythoné‡Œé¢ä¸“é—¨ç”¨äºæ•°æ®åˆ†æçš„å·¥å…·åŒ…ã€‚ä¸ªäººè¿˜è›®æ¨èè¿™æœ¬e-bookçš„Python for Data Analysis ï½12345678import pandas as pdipl_data = &#123;'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings', 'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'], 'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2], 'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017], 'Points':[876,789,863,673,741,812,756,788,694,701,804,690]&#125;df = pd.DataFrame(ipl_data) 1print(df) Team Rank Year Points 0 Riders 1 2014 876 1 Riders 2 2015 789 2 Devils 2 2014 863 3 Devils 3 2015 673 4 Kings 3 2014 741 5 kings 4 2015 812 6 Kings 1 2016 756 7 Kings 1 2017 788 8 Riders 2 2016 694 9 Royals 4 2014 701 10 Royals 1 2015 804 11 Riders 2 2017 690 1print(df.groupby('Team')) &lt;pandas.core.groupby.groupby.DataFrameGroupBy object at 0x10be6c470&gt; 1print(df.groupby('Team').groups) # æŸ¥çœ‹åˆ†ç»„æƒ…å†µ {&apos;Devils&apos;: Int64Index([2, 3], dtype=&apos;int64&apos;), &apos;Kings&apos;: Int64Index([4, 6, 7], dtype=&apos;int64&apos;), &apos;Riders&apos;: Int64Index([0, 1, 8, 11], dtype=&apos;int64&apos;), &apos;Royals&apos;: Int64Index([9, 10], dtype=&apos;int64&apos;), &apos;kings&apos;: Int64Index([5], dtype=&apos;int64&apos;)} 12345grouped = df.groupby('Team')# è¿­ä»£éå†åˆ†ç»„for name_team,group in grouped: print(name_team) print(group) Devils Team Rank Year Points 2 Devils 2 2014 863 3 Devils 3 2015 673 Kings Team Rank Year Points 4 Kings 3 2014 741 6 Kings 1 2016 756 7 Kings 1 2017 788 Riders Team Rank Year Points 0 Riders 1 2014 876 1 Riders 2 2015 789 8 Riders 2 2016 694 11 Riders 2 2017 690 Royals Team Rank Year Points 9 Royals 4 2014 701 10 Royals 1 2015 804 kings Team Rank Year Points 5 kings 4 2015 812 1print(grouped.get_group('Kings')) Team Rank Year Points 4 Kings 3 2014 741 6 Kings 1 2016 756 7 Kings 1 2017 788 12import numpy as npprint(grouped['Points'].agg([np.mean, np.sum])) mean sum Team Devils 768.000000 1536 Kings 761.666667 2285 Riders 762.250000 3049 Royals 752.500000 1505 kings 812.000000 812 12# æŸ¥çœ‹æ¯ä¸ªåˆ†ç»„çš„å¤§å°print(grouped.agg(np.size)) Rank Year Points Team Devils 2 2 2 Kings 3 3 3 Riders 4 4 4 Royals 2 2 2 kings 1 1 1 123# filter some datafilter = df.groupby('Team').filter(lambda x: len(x) &gt;= 3)print(filter) Team Rank Year Points 0 Riders 1 2014 876 1 Riders 2 2015 789 4 Kings 3 2014 741 6 Kings 1 2016 756 7 Kings 1 2017 788 8 Riders 2 2016 694 11 Riders 2 2017 690 12# conclude# df.groupby ä¸»è¦ç”¨äºåˆ†å‰²å¯¹è±¡ï¼Œåº”ç”¨å‡½æ•°ã€Œèšåˆï¼Œè½¬æ¢ï¼Œè¿‡æ»¤ã€ç­‰]]></content>
      <categories>
        <category>ç£¨åˆ€ä¸è¯¯ç æŸ´å·¥</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[å †æ ˆstack & é˜Ÿåˆ—queue(I)]]></title>
    <url>%2F2019%2F06%2F05%2Fbasis%20of%20Data%20Structure%2F</url>
    <content type="text"><![CDATA[Stack Stack ADT: A list with the restriction that insertion and deletion can be performed only from one end, called Top ä¸¾å‡ ä¸ªç®€å•çš„ä¾‹å­æ¥ç†è§£ä¸€ä¸‹å †æ ˆçš„propertyã€Œ Last In First Out ã€ é£Ÿå ‚çš„é¤ç›˜ï¼Œæ”¾åœ¨æœ€ä¸Šé¢çš„æœ€å…ˆè¢«æ‹¿èµ° ä»¥åŠ è¾“å…¥ç¼–è¾‘å™¨é‡Œé¢çš„Undo (æ’¤é”€) æ“ä½œ æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹push (x) &amp; pop ( ) çš„ä¼ªä»£ç å’Œç¤ºæ„å›¾ï¼Œæ¥åŠ æ·±å¯¹è¿™ä¸¤ä¸ªæ“ä½œçš„ç†è§£ã€‚ 1234567# pushif Top = MaximumSize: return (&quot;Stack is full&quot;)else: Top = Top + 1 ArrayStack(Top) = new item 12345# pop if Top = 0: return (&quot;Stack is empty&quot;)else: Top = Top - 1 ç„¶åæˆ‘ä»¬å†æ¥çœ‹ä¸€ä¸‹å †æ ˆçš„åº”ç”¨ï¼Œ åº”ç”¨1: Infix Expression to Postfix Expressionã€‚ã€Œå‚è€ƒè¿™ä¸ªè§†é¢‘è¿›è¡Œæ•´ç†å“’ã€ã€‚ ä¹Ÿå°±æ˜¯æŠŠa op b å˜æˆ ab op æ —å­ğŸŒ° Infix: A + B C - D E Postfix: ABC + DE - æ —å­ğŸŒ° Infix: ((A + B)C - D) E Postfix: AB+C D - E å †æ ˆé‡Œé¢çš„æ“ä½œç¬¦ï¼ˆoperatorï¼‰çš„pushå’Œpopæ˜¯å’Œå…¶precedenceï¼ˆä¼˜å…ˆçº§ï¼‰ç›¸å…³ã€‚é€šä¿—ç‚¹è®²å°±æ˜¯å…ˆä¹˜é™¤ååŠ å‡ğŸ¶ ç”¨pythonæ¥å®ç°ä¸€ä¸‹è¿™ä¸ªä¸œè¥¿ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Stack: def __init__(self): self.items = [] self.length = 0 def push(self, val): self.items.append(val) self.length += 1 def empty(self): return self.length == 0 def pop(self): if self.empty(): return None self.length -= 1 return self.items.pop() def peek(self): if self.empty(): return None return self.items[0] def __str__(self): return str(self.items)precedence = &#123;&apos;*&apos;:3,&apos;/&apos;:3,&apos;+&apos;:2,&apos;-&apos;:2,&apos;(&apos;:1&#125; # dict to store the operators # æ•°å­—è¶Šå¤§ï¼Œä¼˜å…ˆçº§è¶Šé«˜def convert(expression): print(__convert(expression.split()))def __convert(tokens): stack = Stack() res = [] for token in tokens: if token.isalpha(): res.append(token) elif token == &apos;(&apos;: stack.push(token) elif token == &apos;)&apos;: while True: temp = stack.pop() if temp is None or temp == &apos;(&apos;: break elif not temp.isalpha(): res.append(temp) else: if not stack.empty(): temp = stack.peek() while not stack.empty() and precedence[temp] &gt;= precedence[token] and token.isidentifier(): res.append(stack.pop()) temp = stack.peek() stack.push(token) while not stack.empty(): res.append(stack.pop()) return resconvert(&quot;A * ( B + C ) + D&quot;)# [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;+&apos;, &apos;D&apos;, &apos;+&apos;, &apos;*&apos;] åº”ç”¨2: Depth First Search (DFS) ã€‚ã€Œå‚è€ƒè¿™ä¸ªç½‘ç«™ è¿›è¡Œæ•´ç†å“’ã€ã€‚ DFS traverses a graph in depthward motion and uses a stack to remember to get the next vertex to start a search , when a dead end occurs in any iteration. Rule: Step 1: Visit the adjacent unvisited vertex. Mark it as visited, display it and push it in stack. Then we explore each adjacent vertex that is not included in the visited set. Step 2: If no adjacent vertex is found, pop up a vertex from the stack. Step 3: Repeat Step 1 &amp; Step 2 until the stack is empty. ç„¶åæˆ‘ä»¬å†ç”¨pythonæ¥å®ç°ä¸€ä¸‹dfsè¿™ä¸ªæ€æƒ³ã€‚ 123456789101112131415161718# non-recursivedef dfs_iterative(graph, start): visited, stack = set(), [start] # set to keep track of visited vertices while stack: vertex = stack.pop() if vertex not in visited: visited.add(vertex) stack.extend(graph[vertex] - visited) return visited # recursive def dfs_recursive(graph, start, visited = None): if visited is None: visited = set() visited.add(start) for n in graph[start] - visited: dfs_recursive(graph, n, visited) return visited Queue Queue ADT: a queue is open at both its end.{ä¸ªäººè§‰å¾—è¿™å¥è¯ç‰¹åˆ«é‡è¦ï¼Œstackåªæœ‰ä¸€è¾¹å¼€çš„ï¼Œæ‰€ä»¥åé¢è¿›æ¥çš„å…ˆå‡ºå»ã€‚è€Œqueueå°±ä¸ä¸€æ ·äº†ï¼Œä¸¤è¾¹éƒ½æ˜¯å¼€çš„ï¼Œæ‰€ä»¥ä¹Ÿæ‰æœ‰First In First Out è¿™ä¸ªproperty}. One end is always used to insert data ï¼Œä¹Ÿå°±æ˜¯rear or tail è¿™ä¸€ç«¯è¿›è¡Œenqueueè¿™ä¸ªæ“ä½œã€‚The other is used to remove dataï¼Œä¹Ÿå°±æ˜¯front or head è¿™ä¸€ç«¯è¿›è¡Œdequeueè¿™ä¸ªæ“ä½œã€‚ä¸¾ä¸ªç”Ÿæ´»ä¸­çš„ä¾‹å­ï¼šæ¯”å¦‚åœ¨æœºåœºå¤–é¢æ‰“çš„å£«çš„æ—¶å€™ï¼Œå…ˆæ¥çš„å…ˆèµ°å‘€ï½ğŸ¶ Breath First Search algorithm traverses a graph in a breathward motion and uses a queue to remember to get the next vertex to start a search, when a dead end occurs in any iteration.]]></content>
      <categories>
        <category>ç£¨åˆ€ä¸è¯¯ç æŸ´å·¥</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Sorting Algorithmï¼ˆIII)]]></title>
    <url>%2F2019%2F05%2F30%2Ftree%2F</url>
    <content type="text"><![CDATA[è¿™æ¬¡æ•´ç†çš„ç¬”è®°ä¸»è¦å†…å®¹æ˜¯heap sortï¼Œä½†åœ¨è¯´heap sortä¹‹å‰ï¼Œå…ˆè¯´ä¸€ä¸‹æ ‘çš„ç»“æ„å’Œheapçš„ç»“æ„ã€‚ Tree Data Structure æ•°ç»„ã€é“¾è¡¨ã€å †æ ˆã€é˜Ÿåˆ—éƒ½æ˜¯çº¿æ€§ç»“æ„ï¼Œè€Œæ ‘ğŸŒ²æ˜¯éçº¿æ€§ç»“æ„çš„ã€‚æ‰€ä»¥å¦‚æœæ˜¯ä¸€äº›hierarchical dataï¼Œå°±ç”¨æ ‘è¿™ç§æ•°æ®ç»“æ„æ¥å­˜å‚¨ï¼Œæ¯”è¾ƒåˆé€‚ã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œæ ‘çš„é«˜åº¦å’Œæ—¶é—´å¼€é”€æ˜¯æœ‰å…³çš„ï¼Œå¦‚æœæ ‘çš„é«˜åº¦è¶Šå°ï¼Œå¸¸è§„operationçš„æ—¶é—´å¼€é”€ä¹Ÿå°±è¶Šå°ã€‚æˆ‘ä»¬æ¥å…³æ³¨ä¸€ä¸‹height: Height of the empty tree = -1 Height of tree with one node = 0 keep tree balance &lt;â€”â€” make it dense and minimize its height 1234567 root / \... home / \ ugrad course / / | \ ... cs101 cs112 cs113 åƒbinary treeå¯ä»¥åˆ†ä¸º fully binary tree / complete binary tree / balanced binary tree / binary search tree. ç®€å•æåŠä¸€ä¸‹è¿™äº›ä¸åŒç§ç±»çš„binary treeçš„æ€§è´¨ã€‚Fully Binary Tree æ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½æœ‰ 0 / 2 ä¸ªå­èŠ‚ç‚¹; Complete Binary Tree æ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½æœ‰ 2 ä¸ªå­èŠ‚ç‚¹ {the nodes are as left as possible}; Balanced Binary Tree çš„é«˜åº¦æ˜¯ $log_2 n$ ; Binary Search Tree (BST) çš„ left child çš„å€¼è¦å°äºrootçš„å€¼ï¼Œright child çš„å€¼è¦å¤§äºrootçš„å€¼ã€‚BST çš„search(x) / insert(x) / remove(x)çš„average-case running timeéƒ½æ˜¯O(logn), å› ä¸ºæ¯æ¬¡æ¯”è¾ƒçš„æ—¶å€™search spaceéƒ½æ˜¯å¯¹åŠå¯¹åŠçš„å‡å°‘çš„ï½ğŸ˜ ä¸‹é¢å­¦ä¹ ä¸€ä¸‹æ ‘çš„æ„å»ºä»¥åŠéå†ã€‚å¸¸è§çš„depth first traversalçš„æ–¹å¼æœ‰ä¸‰ç§ï¼šin-order traversal (å·¦å­æ ‘ã€æ ¹èŠ‚ç‚¹ã€å³å­æ ‘) ï¼Œ pre-order traversal (æ ¹èŠ‚ç‚¹ã€å·¦å­æ ‘ã€å³å­æ ‘) ï¼Œpost-order traversal (å·¦å­æ ‘ã€å³å­æ ‘ã€æ ¹èŠ‚ç‚¹) 123456789101112131415161718192021222324252627class Node: def __init__(self, key): self.left = None self.right = None self.val = key# tree traversaldef Inorder(root): if root: Inorder(root.left) print(root.val) Inorder(root.right)def Postorder(root): if root: Postorder(root.left) Postorder(root.right) print(root.val)def Preorder(root): if root: print(root.val) Preorder(root.left) Preorder(root.right) å¦‚æœç»“åˆpost-orderå’Œin-orderä¸¤ä¸ªarr[ ] æ¥æ„é€ ä¸€æ£µæ ‘ï¼Œæ€è·¯æ˜¯ã€‚post[ ]çš„æœ€åä¸€ä¸ªèŠ‚ç‚¹ä¸ºroot, åœ¨in[ ]ä¸­æ‰¾åˆ°rootçš„indexï¼Œrootçš„å·¦è¾¹éƒ½ä¸ºå·¦å­æ ‘ï¼Œå³è¾¹éƒ½ä¸ºå³å­æ ‘ã€‚ç±»ä¼¼ä¸‹é¢ï¼Œç„¶ååˆ†åˆ«åœ¨å·¦å­æ ‘å’Œå³å­æ ‘é‡Œé¢ä¸æ–­recursivelyã€‚ 123 1 / \[4, 8, 2, 5] [6, 3, 7] é™¤äº†DFS è¿˜æœ‰ BFS (Breadth First Traversal) / Level Orderã€‚BFS starts visiting nodes from root while DFS starts visiting nodes from leavesã€‚ä¸‹é¢ğŸ‘‡è¿™ä¸ªå›¾å¯ä»¥æ¯”è¾ƒç›´è§‚çš„å¸®åŠ©ç†è§£ã€‚å¦‚æœæ˜¯BFSçš„æ–¹å¼è¿›è¡Œéå†çš„è¯ï¼Œå¾—åˆ°çš„å°±æ˜¯12345 æ’ä¸€å¥é¢˜å¤–è¯ï¼ŒBFSç»å¸¸åœ¨graphç”¨ï¼Œçœ‹ä¸€ä¸ªgraph traversal pseudo code BFS(g, s) create a queue Q Q.enqueue (s) mark s as visited while (Q is not empty): v = Q.dequeue // dequeue the top element for all neighbors w of v in graph G if w is not visited Q.enqueue( w ) mark w as visited Heap A heap is a special tree-based data structure in which the tree is a complete binary tree.{æ€§è´¨ä¸Šé¢ğŸ‘†æœ‰æåˆ°å“ˆï½} æ­¤å¤–heap é€šå¸¸è¢«ç§°ä¸º Priority queueã€‚ Queue å…è®¸çš„æ“ä½œæ˜¯å…ˆè¿›å…ˆå‡ºï¼Œåœ¨é˜Ÿå°¾æ’å…¥å…ƒç´ ï¼Œåœ¨é˜Ÿå¤´å–å‡ºå…ƒç´ ã€‚Heapä¹Ÿæ˜¯åœ¨heapçš„æœ«ç«¯æ’å…¥å…ƒç´ ï¼Œå¼¹å‡ºrootï¼Œä½†heapä¸­å…ƒç´ çš„æ’åˆ—æ˜¯æŒ‰ç…§ä¸€å®šçš„ä¼˜å…ˆé¡ºåºè¿›è¡Œæ’åˆ—çš„ã€‚ heapé€šå¸¸å¯ä»¥åˆ†ä¸ºä¸¤ç±»Max-heap å’Œ Min-heapã€‚è¿™ä¸ªæ˜¯ç”±heapçš„æ€§è´¨å†³å®šçš„ï¼Œå¦‚æœä»»ä¸€èŠ‚ç‚¹çš„å€¼æ˜¯å…¶å­æ ‘æ‰€æœ‰èŠ‚ç‚¹çš„æœ€å¤§å€¼ï¼Œå°±æ˜¯max-heapã€‚å¦‚æœæˆ‘ä»¬è¿™æ—¶å€™extract maxå°±æ˜¯å–å‡ºrootï¼Œä½†è¿™ä¸ªæ—¶å€™ä¸ºäº†ä¿è¯heapçš„å†…éƒ¨é¡ºåºï¼Œæˆ‘ä»¬éœ€è¦max-heapifyã€‚åŒç†å¦‚æœæ’å…¥ä¸€ä¸ªæ–°èŠ‚ç‚¹çš„è¯ï¼Œå¦‚æœæ­¤æ—¶çš„çˆ¶ç»“ç‚¹æ¯”è‡ªå·±å°ï¼Œå°±è¦äº¤æ¢æ–°èŠ‚ç‚¹å’Œçˆ¶ç»“ç‚¹çš„ä½ç½®ã€‚ä¸æ–­å¾€ä¸Šå¯»æ‰¾ï¼Œä¸€ç›´åˆ°æ‰€æœ‰èŠ‚ç‚¹éƒ½æ»¡è¶³æ¡ä»¶ä¸ºæ­¢ã€‚ é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹binary heapçš„ç´¢å¼•ï¼Œrootçš„indexæ˜¯1ï¼Œå‡è®¾nodeçš„indexæ˜¯pï¼Œparent[node]çš„indexæ˜¯p//2ï¼Œleft_childçš„indexæ˜¯2pï¼Œright_childçš„indexæ˜¯2p+1 12345 parent / Node / \L_child R_child ç„¶åæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹max-heapifyçš„pseudo code 12345678910111213Max-Heapify (A,i) l &lt;â€”â€” left-child index R &lt;â€”â€” right-child index if l &lt; heap_size[A] and A[l] &gt; A[i]: greatest &lt;-- l else: greatest &lt;-- i if r &lt; heap_size[A] and A[r] &gt; A[greatest]: greatest &lt;-- r if greatest != i swap( A[i], A[greatest] ) Max-Heapify(A, greatest) end if]]></content>
      <categories>
        <category>ç£¨åˆ€ä¸è¯¯ç æŸ´å·¥</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Sorting Algorithmï¼ˆII)]]></title>
    <url>%2F2019%2F05%2F29%2Fsorting%20algorithm%2F</url>
    <content type="text"><![CDATA[1. Divide and Conquer Algorithmé€šå¸¸è§£å†³é—®é¢˜åˆ†ä¸ºä¸‰æ­¥ï¼š1. Divide æŠŠå¤§é—®é¢˜æ‹†è§£ä¸ºåŒæ ·ç±»å‹çš„å°é—®é¢˜ 2. Conquer recursivleyè§£å†³è¿™äº›å°é—®é¢˜ 3. Combine æŠŠä¹‹å‰è·å¾—çš„å­é—®é¢˜çš„è§£ç»™ç»“åˆèµ·æ¥ã€‚(æ‰€ä»¥é€šå¸¸a divide-and -conquer algorithm makes multiple recursive calls). åœ¨sorting algorithmé‡Œé¢ç”¨åˆ°divide and conquerçš„ä»£è¡¨æ–¹æ³•æœ‰ merge sort å’Œ quick sortã€‚è¿™ä¸¤ç§æ–¹æ³•çš„ running timeæ¯”å‰é¢æåˆ°è¿‡çš„Bubble sortï¼ŒInsertion sortï¼ŒSelection sortè¦å°‘ã€‚ Merge sort : Worst-Case running time $O(nlgn)$ Average-Case running time $O(nlgn)$ Best-Case running time $O(nlgn)$ Quick sort: Worst-Case running time $O(n^2)$ Average-Case running time $O(nlgn)$ Best-Case running time $O(nlgn )$ 2. Merge Sort å¦‚æœarr[ ] ä¸­åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œé‚£å°±æ˜¯å·²ç»sortedï¼Œreturn Divide takes a recursive approach to divide the problem until no sub-problem. In merge sort, we divide the list recursively into two halves until the sub-array has only 1 element. n â€”â€”n/2 â€”â€”n/4 â€”â€” n/8 Divide only takes $O(1)$ time. merge the smaller lists into new list in sorted order. mergeçš„æ–¹å¼æ˜¯conquer the sublists together 2 at a time to produce new sorted sublists until all elements have been fully merged into a single sorted array. The merge function runs in $O(n)$ when merging n elements and the merge sort runs in $O(nlg n )$ time. é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹$O(nlg n)$ æ˜¯æ€ä¹ˆå¾—åˆ°çš„ï½ æƒ³è±¡ä¸€ä¸‹æœ‰ä¸€ä¸ªæ ‘ğŸŒ²ä»ä¸Šåˆ°ä¸‹çš„è¿‡ç¨‹ä¸­ä¸æ–­çš„äºŒåˆ†äºŒåˆ†äºŒåˆ†ï¼Œæˆ‘ä»¬å¼€å§‹çš„ç¬¬ä¸€ä¸ªlevelæœ‰nä¸ªå…ƒç´ ï¼Œæœ€åä¸€ä¸ªlevel æ¯ä¸€ä¸ªéƒ½åªæœ‰ä¸€ä¸ªå…ƒç´ ã€‚ä¸è¿‡æ¯ä¸€ä¸ªlevelçš„å…ƒç´ ä¸ªæ•°æ€»å’Œéƒ½æ˜¯nã€‚merge sortçš„æ€»æ—¶é—´ç­‰äºå¯¹mergeæ¯ä¸€å±‚çš„æ—¶é—´æ±‚å’Œ (sum)ï¼Œå‡è®¾æ¯ä¸€å±‚Merge sortæ‰€éœ€è¦çš„æ—¶é—´æ˜¯cn,(cç›¸å½“äºä¸€ä¸ªconstant) ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬æœ‰Lå±‚çš„è¯ï¼Œæˆ‘ä»¬æœ€ç»ˆå¯ä»¥å¾—åˆ° L*cn ã€‚æ ¹æ®binary treeçš„æ€§è´¨æˆ‘ä»¬çŸ¥é“ $ L = log_2(n) + 1 $ æ‰€ä»¥merge sortçš„æ—¶é—´æ˜¯$cn(log_2n)$ ,æ‰€ä»¥æˆ‘ä»¬å¾—åˆ°äº†$O(nlg n)$ç„¶åä¸¾ä¸ªä¾‹å­æ¥å¸®åŠ©ç†è§£Merge Sort. æˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªæœªæ’åºçš„æ•°ç»„ arr[ ] = [14, 33, 27, 10, 35, 19] é¦–å…ˆæˆ‘ä»¬è¦divide è¿™ä¸ªæ•°ç»„ã€‚ Left_arr[ ] = [14 33 27] Right_arr[ ] = [10 35 19] Left_sub_arr1 = [14] Left_sub_arr2 = [33 27] Right_sub_arr1 = [10] Right_sub_arr2 = [35 19] , å†ä¸€æ¬¡è¿›è¡Œdivideï¼Œæœ€åæˆ‘ä»¬å¾—åˆ°çš„ sub_array = 14\ 33\ 27\10\35\19 ç°åœ¨æˆ‘ä»¬è¦è¿›è¡Œmerge. Left_arr_merge1 = [14] Left_arr_merge2 = [27,33], Right_arr_merge1 = [10] Right_arr_merge2 = [19 35] Left_arr = [14,27,33] Right = [10 19 35] Sorted_arr = [10, 14, 19, 27, 33, 35] é™„ä¸Špythonå®ç°çš„ä»£ç  ğŸ¤­ 12345678910111213141516171819202122232425262728293031def mergeSort(arr): print("original array is", arr) if len(arr) &gt; 1: mid = len(arr) // 2 lefthalf = arr[:mid] righthalf = arr[mid:] mergeSort(lefthalf) mergeSort(righthalf) i = 0 j = 0 k = 0 while i &lt; len(lefthalf) and j &lt; len(righthalf): if lefthalf[i] &lt; righthalf[j]: arr[k] = lefthalf[i] i = i + 1 else: arr[k] = righthalf[j] j = j + 1 k = k + 1 while i &lt; len(lefthalf): arr[k] = lefthalf[i] i = i + 1 k = k + 1 while j &lt; len(righthalf): arr[k] = righthalf[j] j = j + 1 k = k + 1 print("merging", arr) Quick Sorté¦–å…ˆè¦æ¶‰åŠåˆ°Pivot valueçš„é€‰æ‹©ï¼Œå› ä¸ºPivot value å’Œpartition æœ‰å…³ï¼Œpartition åˆæ˜¯ quick sortç®—æ³•å®ç°è¿‡ç¨‹çš„æ ¸å¿ƒã€‚ Always pick first element as pivot {åªæ˜¯ä¸ºäº†ç®€å•åŒ–} Put pivot at its correct position in sorted array and put all smaller elements (smaller than pivot) before pivot, and put all greater elements (greater than pivot) after pivot. {Done in the linear time} ç„¶åæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹quick sortçš„ä¼ªä»£ç  1234567891011 quickSort( arr[ ], start_idx, end_idx)&#123; if ( start_idx &lt; end_idx) pivot = partition(arr, start_idx, end_idx) quickSort(arr, start_idx, pivot - 1) qucikSort(arr, pivot + 1, high) &#125; æˆ‘ä»¬å†æ¥è¿›è¡Œä¸€ä¸‹time complexityçš„åˆ†æï½ its worst-case running time is as bad as selection sortâ€™s and insertion sortâ€™s: $O(n^2 )$ ã€‚å‚è€ƒå¯æ±—å­¦é™¢çš„algorithmæ•™ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥çŸ¥é“worst-caseå°±æ˜¯the most unbalanced partitions ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ åŒç†ï¼Œbest case occurs when the partitions are evenly balanced as possible. å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ ä½†æ˜¯quick sortçš„average-case running time æ˜¯å’Œmerge-sortä¸€æ ·çš„ï¼Œéƒ½æ˜¯$O(nlg n)$. In practice, quicksorts outperforms merge sort. Difference between Merge Sort and Quick SortIn Merge Sort, divide å‡ ä¹æ²¡èµ·åˆ°ä»€ä¹ˆä½œç”¨ï¼Œä¸»è¦èµ·ä½œç”¨çš„éƒ¨åˆ†æ˜¯mergeéƒ¨åˆ†ã€‚ã€Œmergesortã€å°±æ˜¯åœ¨mergeéƒ¨åˆ†âœ…ï½ In Quick Sort, merge å‡ ä¹æ²¡èµ·åˆ°ä»€ä¹ˆä½œç”¨ï¼Œä¸»è¦èµ·ä½œç”¨çš„éƒ¨åˆ†æ˜¯divideéƒ¨åˆ†ã€‚ã€Œå’Œpivotæ¯”è¾ƒå¤§å°ï¼Œä»è€Œåˆ†å‰²ã€]]></content>
      <categories>
        <category>ç£¨åˆ€ä¸è¯¯ç æŸ´å·¥</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Sorting Algorithmï¼ˆI)]]></title>
    <url>%2F2019%2F05%2F29%2FSorting_Algorithm%2F</url>
    <content type="text"><![CDATA[å› ä¸ºä¹‹å‰æ²¡æœ‰ç³»ç»Ÿå­¦data structure and algorithmï¼Œæš‘å‡äº¡ç¾Šè¡¥ç‰¢ä¸€ä¸‹ï¼ˆğŸ¤¦â€â™€ï¸ æŠŠè¿™äº›å¸¸è§„çš„éƒ½ç”¨pythonç»™å®ç°ä¸€éã€‚å…ˆä»sortingå¼€å§‹ï¼Œsortingå¯ä»¥åˆ†ä¸ºinternal sortingå’Œexternal sortingã€‚external æŒ‡çš„æ˜¯ all data cannot be placed in-memory at a time, å¸¸è§çš„external sorting algorithmæœ‰merge-sortã€‚ä¸è¿‡any sorting algorithm can be used to sort the data that has been loaded into memoryã€‚ ä¸‹é¢ä¸¾çš„ä¾‹å­éƒ½å‡è®¾arrayæ˜¯æŒ‰ç…§å‡åºè¿›è¡Œæ’åˆ—çš„ã€‚ğŸ˜†æ­¤å¤– Running time is an important thing to consider when selecting a sorting algorithm since efficiency is often though of in terms of speedã€‚æ‰€ä»¥éƒ½å°è¯•åˆ†æä¸€ä¸‹best time complexityå’Œworst time complexityã€‚ Bubble Sort (internal sorting) Bubble Sort æ˜¯æœ¬ç§‘æ¥è§¦çš„ç¬¬ä¸€ä¸ªâ˜ï¸æ’åºç®—æ³•ï¼Œ Bubble sort steps through the list and compares adjacent pairs of elements. The elements are swapped if there are in the wrong order. ä½†æ˜¯ä¹Ÿæ­£æ˜¯å› ä¸ºå†’æ³¡æ³•ä»å¤´åˆ°å°¾çš„éå†è¿‡ç¨‹ä¸­éœ€è¦ä¸¤ä¸¤ç›¸æ¯”è¾ƒï¼Œå¦‚æœæ¯”è¾ƒçš„å¾ˆé¡ºåˆ©ï¼Œä¹Ÿå°±æ˜¯æ•´ä¸ªarrayå…¶å®å·²ç»æ˜¯æ’åºå¥½äº†çš„ï¼Œè¿™æ—¶å€™çš„time complexity å°±æ˜¯$o(n)$ï¼Œå¦‚æœæ•´ä¸ªarrayæ˜¯ä¹±ä¸ƒå…«ç³Ÿçš„ï¼Œè¿™æ—¶å€™ä¹Ÿå°±æ˜¯worst time complexity $o(n^2)$ 12345678def bubbleSort(arr): n = len(arr) # traverse through all array elements for i in range(n): for j in range(0, n - i - 1): # traverse the array from 0 to n-i-1 if arr[j] &gt; arr[j + 1]: arr[j], arr[j + 1] = arr[j + 1], arr[j] Insertion Sort (internal sort) On each loop iteration, insertion sort removes one element from the array. It then finds the location where that element belongs within another sorted array and inserts it there. æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸‹æ’å…¥æ’åºçš„pesudo codeã€‚ Loop from i = 1 to n-1 â€‹ pick element arr[i], and insert it into the sorted sequence arr[0...i-1] ç„¶åå†ä¸¾ä¸ªä¾‹å­æ¥ç†è§£ï¼Œ arr[] = 11 13 10 20 1st loop: 11 13 10 20 ï¼ˆsince 11&lt; 13) 2nd loop: 10 11 13 20 (since 10 &lt; 11) 3rd loop: 10 11 13 20 (since 20 &gt; 13) ç°åœ¨æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹time complexityã€‚å¥½çš„æƒ…å†µå°±æ˜¯æ²¡æœ‰swapçš„è¿‡ç¨‹ï¼Œå’Œä¸Šé¢bubbleä¸€æ ·ï¼Œæ•´ä¸ªarrayå…¶å®å·²ç»æ˜¯æ’åºå¥½äº†çš„ï¼Œæ‰€ä»¥best time complexity $o(n)$ ã€‚åçš„æƒ…å†µå°±æ˜¯æˆ‘ä»¬éœ€è¦çš„æ˜¯å‡åºçš„ï¼Œä½†ä¸€å¼€å§‹çš„arrayæ˜¯é™åºæ’åˆ—çš„, # opertaion = n(n-1) æ‰€ä»¥worst time complexity $o(n^2)â€‹$ 12345678def insertion_sort(arr): for i in range(1, len(arr)): key = arr[i] j = i - 1 while j &gt;= 0 and key &lt; arr[j]: arr[j], arr[j + 1] = arr[j + 1], arr[j] j = j - 1 return arr Selection Sort (internal sort) The algorithm maintains two subarrays in a given array. 1)The subarray which is already sorted 2) Remaining subarray which is unsortedã€‚ä¹Ÿå°±æ˜¯æˆ‘ä»¬æŠŠä¸€ä¸ªæ•°ç»„åˆ†ä¸ºå·²ç»æ’åˆ—æ•´é½çš„ å’Œ ä¹±ä¸ƒå…«ç³Ÿçš„ä¸¤ç±»ã€‚åœ¨ä¹±ä¸ƒå…«ç³Ÿçš„é‚£ä¸€éƒ¨åˆ†æ‰¾åˆ°ä¸€ä¸ªæœ€å°çš„å…ƒç´ ï¼ŒæŠŠå®ƒæ”¾åˆ°å·²ç»æ’åˆ—å¥½çš„é‚£ä¸€éƒ¨åˆ†çš„æ•°ç»„ä¸­æ­£ç¡®çš„ä½ç½®ä¸­å»ã€‚ ç”¨ä¸Šé¢åŒæ ·çš„ä¾‹å­æ¥ç†è§£ä¸€ä¸‹ arr[] = 11 13 10 20 10 // find the minimum element in arr[0â€¦3] put at the beginning 10// sorted 11 13 20// unsorted 11 // find the minimum element in arr[1â€¦3] put at the second place 10 11 //sorted 13 20 // unsorted 13 // find the minimum element in arr[2..3] put at the third place then we get 10 11 13 20 12345678def selection_sort(arr): for i in range(len(arr)): minimal = i for j in range(i + 1, len(arr)): if arr[j] &lt; arr[minimal]: minimal = j arr[minimal], arr[i] = arr[i], arr[minimal] return arr]]></content>
      <categories>
        <category>ç£¨åˆ€ä¸è¯¯ç æŸ´å·¥</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[plotting]]></title>
    <url>%2F2019%2F01%2F13%2Fplotting%2F</url>
    <content type="text"><![CDATA[Hello, 2019å¹´çš„ç¬¬ä¸€ç¯‡åšå®¢ã€‚ä¾æ—§æ˜¯ä»‹ç»ä¸€ä¸‹æˆ‘æœ€è¿‘ä½¿ç”¨çš„å‡ ä¸ªå¾ˆå¥½ç”¨çš„å·¥å…·ã€‚ ç™¾åº¦å¼€æºçš„pyechartsã€‚ ä¹‹å‰ã€Œæœºå™¨ä¹‹å¿ƒã€è®¤è¯†çš„å°ä¼™ä¼´weng jjæ¨èç»™æˆ‘çš„ã€‚å¼•ç”¨pyechartså®˜ç½‘æ–‡æ¡£çš„ä»‹ç»ï¼Œpyechartsç›¸å½“äºpythonç‰ˆæœ¬çš„Echartsï¼Œæ˜¯ç™¾åº¦å¼€æºçš„ä¸€ä¸ªæ•°æ®å¯è§†åŒ–JSåº“ã€‚å¯ä»¥ç”»é¥¼çŠ¶å›¾ã€ç«ç‘°å›¾ã€æŠ˜çº¿å›¾ç­‰å¤šç§ç±»å‹çš„å›¾ã€‚ ä¸‹é¢ä¸¾ä¸€ä¸ªæˆ‘åšçš„ä¸€ä¸ªå¾ˆç®€å•çš„ä¾‹å­ã€‚æ•°æ®æ¥æºæ˜¯kaggleä¸Šåˆ†ææ³•å›½åœ°åŒºä¸å¹³ç­‰çš„æƒ…å†µï¼Œç»è¿‡äº†æ•°æ®æ¸…ç†ä¹‹åï¼ˆæ­¤å¤„çœç•¥ï¼Œå› ä¸ºå¾ˆç®€å•0-oï¼‰ï¼Œæˆ‘ä»¬å¾—åˆ°äº†è¡¨æ ¼ã€‚æ­¤æ—¶æˆ‘ä»¬æƒ³ç”¨è¡¨ä¸­çš„æ•°æ®ç»™å¯è§†åŒ–å‡ºæ¥ï¼Œä»¥ä¾¿æ›´åŠ æ¸…æ¥šçš„çœ‹åˆ°ç”·æ€§å’Œå¥³æ€§ä»¥åŠä¸åŒçš„å·¥ä½œç§ç±»çš„äººåœ¨æ¯å°æ—¶æ‹¿åˆ°çš„å‡€æ”¶å…¥çš„å·®è·å‚è€ƒpyechartæ–‡æ¡£ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸‹é¢çš„å›¾ã€‚åœ¨å…¨æ³•å›½äººå‡æ€»å·¥èµ„æœ€é«˜çš„å‰åä¸ªåœ°åŒºï¼Œä¸åŒæ€§åˆ«æ‰€æ‹¿åˆ°çš„å·¥èµ„çš„å·®å¼‚ã€‚ åœ¨å…¨æ³•å›½ä»äº‹ä¸åŒèŒä¸šçš„äººä¹‹é—´çš„å·¥èµ„å·®å¼‚ã€‚ ä»æŸ±çŠ¶å›¾å’Œä¸Šé¢çš„ç«ç‘°å›¾ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾ˆè½»è€Œæ˜“ä¸¾çš„çœ‹åˆ°å·®è·å•¦ï¼ˆğŸ˜ï¼‰ï¼Œè€Œä¸”pyechartsç”»å‡ºæ¥çš„ğŸ“ˆï¼Œæ¯”pythonå¸¦çš„matplotlibå·¥å…·åŒ…ç”»å‡ºçš„å›¾è¡¨ï¼Œè§†è§‰æ•ˆæœè¦å¥½å¾ˆå¤šã€‚]]></content>
      <categories>
        <category>ç£¨åˆ€ä¸è¯¯ç æŸ´å·¥</category>
      </categories>
      <tags>
        <tag>å·¥å…·</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTex Installation on OSX]]></title>
    <url>%2F2018%2F12%2F19%2Flatex-installation-on-OSX%2F</url>
    <content type="text"><![CDATA[Latexåœ¨è®ºæ–‡å†™ä½œä¸­ç”¨çš„æ¯”è¾ƒå¤šï¼Œæ¯”wordå¥½ç”¨ä¸å°‘ã€‚Latexæœ€å¤§çš„ä¼˜åŠ¿å°±æ˜¯å¤æ‚å…¬å¼çš„ç¼–è¾‘ä¸æ’ç‰ˆéå¸¸æ¼‚äº®ã€‚ä»Šå¤©è®°å½•ä¸€ä¸‹åœ¨mac OSXç¯å¢ƒä¸‹å¦‚ä½•èˆ’æœçš„æ•²Latexã€‚è§£å†³æ–¹æ¡ˆï¼šSublime Text + MacTex + Skim MacTeXæ˜¯TeXLiveçš„Macç‰ˆï¼Œåœ¨http://www.tug.org/mactex/ä¸­ä¸‹è½½MacTex.pkgã€‚ å®‰è£…å®Œæˆåï¼Œä¼šå‡ºç°å¦‚ä¸‹ç•Œé¢ã€‚ Sublime Text æ˜¯ç¼–è¾‘å™¨ï¼Œé¦–å…ˆæŒ‰ã€Œcommand + shift + Pã€æ¥æ‰“å¼€å‘½ä»¤è¡Œï¼Œç„¶åè¾“å…¥å‘½ä»¤ã€ŒInstall Packageã€,æŒ‰ä¸‹Enterå›è½¦é”®ã€‚åœ¨è¾“å…¥å®Œæˆåï¼Œå†è¾“å…¥â€œLaTex Toolsâ€å¹¶å®‰è£…ã€‚æˆ‘ä»¬å¯ä»¥åœ¨å®‰è£…packageçš„è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥æ‰“å¼€consoleã€‚Consoleä¸€èˆ¬åœ¨ST3çš„åº•éƒ¨ï¼Œç”¨æ¥æŸ¥çœ‹å®‰è£…çš„è¿›ç¨‹ã€‚ . Skimæ˜¯åœ¨OSXä¸‹é¢æ¯”è¾ƒè½»ä¾¿çš„ä¸€ä¸ªPDFé˜…è¯»å™¨ã€‚å®‰è£…å®Œé˜…è¯»é˜…è¯»å™¨ï¼Œéœ€è¦ã€ŒSyncã€ä¸€ä¸‹Sublime Textå’ŒSkimï¼Œå¦‚æœä¸‹è½½çš„æ˜¯Sublime Text3å°±é€‰Sublime Textï¼Œå¦‚æœæ˜¯Sublime Text2å°±é€‰æ‹©Sublime Text2ã€‚ PSï¼ŒLaTex Templatesä¸Šæä¾›äº†å¾ˆå¤šç®€å†æ¨¡ç‰ˆï¼Œå¯ä»¥å€Ÿé‰´å‚è€ƒã€‚]]></content>
      <categories>
        <category>ç£¨åˆ€ä¸è¯¯ç æŸ´å·¥</category>
      </categories>
      <tags>
        <tag>å·¥å…·</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FAIRè§†è§‰è®ºæ–‡é›†é”¦]]></title>
    <url>%2F2018%2F12%2F16%2Floss%2F</url>
    <content type="text"><![CDATA[FAIRåœ¨ç›®æ ‡æ£€æµ‹ã€å®åŠ›åˆ†å‰²ç­‰é¢†åŸŸéƒ½åšäº†åˆ›æ–°ã€‚æœ¬ç¯‡åšå®¢ä»R-CNNåˆ°FPNåˆ°RetinaNetåˆ°Mask-RCNNåšäº†ä¸€ç‚¹å½’çº³æ€»ç»“ã€‚ Faster-RCNNé¦–å…ˆæ¥æFaster-RCNN ç½‘ç»œç»“æ„ï¼Œæ˜¯å› ä¸ºMask-RCNNæ˜¯åœ¨å…¶åŸºç¡€ä¸Šæ”¹è¿›ç½‘ç»œç»“æœã€Œæ›´å…·ä½“ç‚¹å¹¶åˆ—åŠ ä¸€ä¸ªmask branchã€è€Œå¾—åˆ°çš„æ¥å®ç°segmentationã€‚Faster-RCNNåœ¨object detectionä¸­ç›¸å½“äºbaseline systemï¼Œä¹Ÿæ˜¯benchmarkã€‚ä¸»è¦åŒ…æ‹¬äº†å¯¹ç›®æ ‡ç‰©ä½“çš„åˆ†ç±»ï¼ˆclassificationï¼‰ï¼Œä»¥åŠç”¨å€™é€‰æ¡†ï¼ˆbounding boxï¼‰æ¥å¯¹å›¾ç‰‡ä¸­çš„ä½ç½®è¿›è¡Œå®šä½ã€‚åœ¨æ­¤ä¹‹å‰ä¹Ÿå·²æœ‰äº†Fast-RCNNä¹‹ç±»çš„ç›®æ ‡æ£€æµ‹ç®—æ³•äº†ã€‚æ–‡ç« ã€ŒFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networksã€çš„åˆ›æ–°åœ¨äºè§£å†³äº†Region Proposalç”Ÿæˆå¼€é”€é—®é¢˜ã€‚å½“ç”Ÿæˆçš„å€™é€‰æ¡†è¿‡å¤šæ—¶ï¼Œprocessing speedä¼šå—åˆ°å½±å“ï¼Œä»è€Œæ²¡æ³•å¾ˆå¥½çš„å®ç°real-time object detectionã€‚ we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals åœ¨Faster-RCNNä¸­ä½¿ç”¨RPNæ¥è¿›è¡Œå€™é€‰æ¡†çš„ç¡®å®šï¼Œå³ã€ŒRegion proposal Networkæ‰¾å‡ºç‰©ä½“å¯èƒ½å­˜åœ¨çš„æ‰€æœ‰ä½ç½®ã€ï¼Œåœ¨è¿™ä¸€ä¸ªè¿‡ç¨‹ä¸­æ‰¾å…¨ï¼Œæ²¡æœ‰æ¼æ£€å¾ˆé‡è¦ï¼Œä¸ç„¶åé¢çš„åˆ†ç±»ä¹Ÿæ²¡æ³•åˆ†äº†ã€‚å³Recallçš„å€¼è¦é«˜ï¼Œã€ŒRecall=æ­£ç¡®è¯†åˆ«å‡ºæ¥çš„object/æ•°æ®åº“é‡Œå«æœ‰çš„objectï¼Œå½“recall=100%æ—¶ï¼Œè¡¨ç¤ºæ²¡æœ‰æ¼æ£€ã€ã€‚RPNç½‘ç»œæ˜¯ä¸€ç§å…¨è¿æ¥ç½‘ç»œï¼ˆFCNåœ¨ä¸‹æ–‡æœ‰æåˆ°å“ˆå“ˆï¼‰ RPNé¢„æµ‹äº†object bounds and objectness scores at each positionï¼Œè¿™ç»™Fast-RCNNèµ·åˆ°ç±»ä¼¼æŒ‡å“ªæ‰“å“ªçš„ä½œç”¨äº†ã€‚æ­¤å¤–ï¼Œè¿™é‡Œä¹Ÿæ˜¯æ–‡ç« çš„å¦ä¸€ä¸ªåˆ›æ–°ç‚¹ï¼Œé€šè¿‡ã€Œsharing the convolutional featuresã€å®ç°äº†RPNå’ŒFast-RCNNèåˆåˆ°ä¸€ä¸ªç½‘ç»œä¸­å»äº†ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æ¥ç†è§£ä¸€ä¸‹ã€Œsharingã€ï¼ŒRPNä»feature map ä¸Šé€‰æ‹©å‡ºäº†ä¸€ç³»åˆ—çš„bounding boxï¼Œç„¶åFast-RCNNå†æ¬¡åˆ©ç”¨äº†feature mapï¼Œå¹¶ç”¨ROI poolingï¼ˆä¸»è¦åŒ…æ‹¬ä¸‰æ­¥ï¼š1. æŠŠ region proposal åˆ†ä¸ºnç­‰åˆ†ï¼Œn=the dimension of the output 2. æ‰¾åˆ°æ¯ä¸ªsectionæœ€å¤§çš„å€¼ 3.æŠŠæ¯ä¸ªæœ€å¤§çš„æå–å‡ºæ¥ä½œä¸ºoutput bufferï¼‰æ¥å¯¹æ¯ä¸ªcandidate boxè¿›è¡Œclassification å’Œ bounding box regressionï¼Œä¹Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸ŠèŠ‚çœäº†è®¡ç®—å¼€é”€ï¼ŒåŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ã€‚ using the recently popular terminology of neural networks with â€œattentionâ€ mechanisms, the RPN component tells the unified network where to look Feature Pyramid NetworkèƒŒæ™¯ï¼šROIæ˜ å°„åˆ°æŸä¸ªfeature mapæ˜¯å°†åº•å±‚çš„åæ ‡ç›´æ¥é™¤ä»¥strideï¼Œæ˜¾ç„¶å¯¹äºå°ç›®æ ‡ï¼ˆsizeæ¯”è¾ƒå°ï¼‰ç‰©ä½“æ¥è¯´ï¼Œåˆ°åé¢çš„å·ç§¯æ± åŒ–æ—¶ï¼Œå®é™…çš„è¯­ä¹‰ä¿¡æ¯å°±ä¸¢å¤±äº†å¾ˆå¤šäº†ã€‚åœ¨CNNä¸­ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘ä¸åŒç§ç±»çš„invarianceæ¥åšè¯†åˆ«ï¼Œscale invarianceå¾ˆéš¾è¢«CNNè€ƒè™‘åˆ°ã€‚ä¸€èˆ¬é€šå¸¸çš„åšæ³•æœ‰ä¸¤ç§Image Pyramidå’ŒFeature Pyramidã€‚å…¶ä¸­Feature Pyramidçš„ä»£è¡¨æœ‰SPPNetå’ŒFPNã€‚ FPNç»“æ„ï¼šFPNæ˜¯åŸºäºç‰¹å¾æå–çš„ç½‘ç»œï¼Œå¯ä»¥æ˜¯ResNetä¹Ÿå¯ä»¥æ˜¯DenseNetã€‚åœ¨Mask-RCNNä¸­å°±å°†ResNetå’ŒFPNç›¸ç»“åˆï¼Œä½œä¸ºbase-networkã€‚å¸¸è§çš„å‘½åæ–¹å¼ï¼šä¸»å¹²ç½‘ç»œ-å±‚æ•°-FPNï¼Œå¦‚ResNet-101-FPNã€‚åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸‹ï¼Œé€‰å–ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å°±å¯ä»¥å®ç°FPNã€‚FPNè®¾è®¡çš„é‡‘å­—å¡”ç»“æ„åŒ…æ‹¬äº†bottom-up &amp; top-down &amp; lateral connectionsä¸‰ç§ç»“æ„ã€‚bottom-upæ˜¯ä¸»å¹²CNNæ²¿å‰å‘ä¼ è¾“ï¼ˆfeed-foward/inference)ã€‚top-downæ˜¯ä¸Šé‡‡æ ·ï¼ˆupsampling) ã€‚lateral connectioné€šå¸¸ä½¿ç”¨ 1x1çš„å·ç§¯ï¼Œèåˆä¸åŒå±‚çš„è¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶é™ä½ç»´åº¦ï¼Œå’Œupsamplingå åŠ åå¾—åˆ°ä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾å›¾ï¼Œä»è€Œè¾¾åˆ°å¤šå°ºåº¦anchorçš„æ•ˆæœã€‚ focal loss for dense object detectionèƒŒæ™¯ï¼šç›®æ ‡æ£€æµ‹ä¸­ æ ¹æ®æœ‰æ ‡ç­¾çš„æ•°æ®åˆ’åˆ† positive / negative training examples(å…¶ä¸­é€šè¿‡ bounding box çš„IOUæ¥ç¡®å®šæ­£è´Ÿæ ·æœ¬ï¼ŒIOUè¶…è¿‡ä¸€å®šçš„é˜ˆå€¼åˆ™ä¸ºæ­£æ ·æœ¬)ä¸€èˆ¬è´Ÿæ ·æœ¬ä¼šå¤šäºæ­£æ ·æœ¬.ä½†æ˜¯ä¸ºäº†è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ä¸åå‘äºnegativeï¼Œéœ€è¦ä¿è¯æ ·æœ¬æ•°ç›®çš„balance.Focal loss for dense object detectionè§£å†³çš„å°±æ˜¯ foreground-backgroundå³å‰æ™¯å’ŒèƒŒæ™¯çš„æ•°æ®çš„imbalanceã€‚ åˆ›æ–°ï¼šæœ¬æ–‡æ˜¯é€šè¿‡ã€Œlossã€æ”¹å˜ä¸ºã€Œfocal lossã€è€Œä¸æ˜¯ã€Œarchitectureã€åˆ›æ–°æ¥speed/accuracy/complexityçš„trade-offï¼Œåœ¨è¿™é‡ŒæŒ‡çš„ä¸€æçš„æ˜¯ä¸»å¹²ç½‘ç»œã€ŒRetinaNetã€ã€‚ Fully Convolutional NetworksFully Convolutional Networks for semantic segmentation ä¸€å¼€å§‹è¯´çš„ï¼š combines semantic information from a ã€Œdeep , coarseã€ layer with appearance information from a ã€Œshallow, fineã€ layers é‚£ä¸ºä»€ä¹ˆdeepå’Œcoarseè¿åœ¨ä¸€èµ·ï¼Œshallowå’Œfineæ¥æ¥åœ¨ä¸€èµ·ï¼Œä¸æ˜¯è¶Šdeepçš„å±‚ï¼Œè¶Šæœ‰è¡¨è¾¾åŠ›ä¹ˆï¼Ÿ å…¨è¿æ¥ç½‘ç»œå’ŒCNNä¹‹é—´çš„åŒºåˆ«ï¼šç»å…¸çš„CNNæ˜¯å°†å·ç§¯å±‚äº§ç”Ÿçš„feature mapä½¿ç”¨å…¨è¿æ¥å±‚æ˜ å°„ä¸ºå›ºå®šé•¿åº¦çš„ç‰¹å¾å‘é‡ï¼Œæœ€åè¾“å‡ºçš„æ˜¯æ¦‚ç‡ã€‚FCNå°†å…¨è¿æ¥å±‚éƒ½å˜åŒ–ä¸ºå·ç§¯å±‚ï¼Œã€ŒE.X.: å°†4096 å˜æˆ1x1x4096ã€æ˜¯é’ˆå¯¹è¯­ä¹‰åˆ†å‰²è®­ç»ƒçš„ä¸€ä¸ªend-to-end, pixelçš„ç½‘ç»œï¼Œæœ€åè¾“å‡ºçš„æ˜¯heatmapçƒ­åŠ›å›¾ã€‚ FCNç½‘ç»œç»“æ„åˆ›æ–°ç‚¹ï¼šFCNå¯ä»¥æ¥å—ä»»æ„å°ºå¯¸çš„è¾“å…¥å›¾åƒï¼Œé‡‡ç”¨åå·ç§¯å±‚å¯¹æœ€åä¸€ä¸ªå·ç§¯å±‚çš„feature mapåšä¸Šé‡‡æ ·upsamplingï¼Œä½¿å…¶æ¢å¤åˆ°è¾“å…¥å›¾åƒçš„ç›¸åŒå°ºå¯¸ï¼Œä»è€Œå¯¹æ¯ä¸€ä¸ªåƒç´ éƒ½äº§ç”Ÿä¸€ä¸ªé¢„æµ‹ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹è¾“å…¥å›¾åƒçš„ç©ºé—´ä¿¡æ¯ã€‚ä½†æ˜¯è¿™æ ·å¾—åˆ°çš„ç»“æœæ¯”è¾ƒcoarser, ä¸€äº›ç»†èŠ‚ä¸èƒ½æ¢å¤ã€‚å› æ­¤ï¼Œä½œè€…é‡‡ç”¨äº†skip architectureæ¥ä¼˜åŒ–ä¸Šé‡‡æ ·ï¼Œå³å°†ä¸åŒæ± åŒ–å±‚çš„ç»“æœè¿›è¡Œä¸Šé‡‡æ ·ï¼Œç„¶åç»“åˆè¿™äº›ç»“æœæ¥ä¼˜åŒ–è¾“å‡ºã€‚ã€ŒE.X ç¬¬äº”å±‚çš„è¾“å‡º32å€æ”¾å¤§åå·ç§¯åˆ°åŸå›¾å¤§å°æ—¶æ¯”è¾ƒç²—ç³™ï¼Œå› æ­¤ä½œè€…å°†ç¬¬å››å±‚è¾“å‡º16å€æ”¾å¤§ï¼Œç¬¬3å±‚è¾“å‡º8å€æ”¾å¤§ï¼Œå¯ä»¥ä»åŸè®ºæ–‡ä¸­æ’å›¾çœ‹åˆ°è¶Šä½æ± åŒ–å±‚ï¼Œè¶Šç²¾ç»†ã€å› æ­¤æˆ‘ä»¬ä¹Ÿå°±å¯ä»¥ç†è§£äº†ä¸Šæ–‡çš„é—®é¢˜ã€‚ FCNåœ¨mask-RCNNä¸­çš„åº”ç”¨ï¼šåœ¨the mask branchä¸­ï¼ŒFCNè¢«ç”¨åœ¨æ¯ä¸ªROIä¸­è¿›è¡Œpixel-to-pixelçš„åˆ†å‰²ï¼Œè¿™ä¹Ÿæ˜¯mask-RCNNè¶…è¶Šäº†Faster-RCNNçš„åœ°æ–¹ã€‚ä½œè€…åœ¨æ–‡ç« é‡Œæ˜¯è¿™ä¹ˆè¯´çš„ï¼š Our method, called Mask-RCNNï¼Œextends Faster-RCNN by adding a branch for predicting segmentation masks on each Region of Interest,in parallel with the existing branch for classification and bounding box regression. Mask-RCNNMask-RCNNè®ºæ–‡åœ°å€ Mask-RCNNå®ç°çš„ä»»åŠ¡è¦æ›´ã€Œéš¾ã€ï¼Œå› ä¸ºä¸å†æ˜¯object detection è€Œæ˜¯è¦è¾¾åˆ°instance segmentationï¼Œç»†åŒ–åˆ°åŒºåˆ†ç±»åˆ«ä¸­çš„ä¸åŒå®ä¾‹ã€‚é€šä¿—ç‚¹è¯´ï¼Œåƒç´ åˆ†ç±»çš„è¯å¯ä»¥ç”¨ä¸åŒçš„é¢œè‰²æ¥åŒºåˆ«ä¸åŒçš„å®ä¾‹ï¼Œä½†æ˜¯å®ä¾‹åˆ†å‰²çš„æ—¶å€™å³ä½¿æ˜¯åŒä¸€ç§ç±»çš„ç‰©ä½“ï¼Œæ¯”å¦‚éƒ½æ˜¯çŒ«çŒ«ï¼Œä¹Ÿè¦åŒºåˆ«å‡ºæ©˜çŒ«å’ŒåŠ è²çŒ«ã€‚åƒFCNä¸­ä¹Ÿå¯ä»¥ç”¨åœ¨å®ä¾‹åˆ†å‰²çš„æƒ…æ™¯ä¸­ï¼Œä½†å®ƒä»¬çš„åšæ³•æ˜¯ï¼Œå¯¹æ¯ä¸ªåƒç´ è¿›è¡Œmulti-class categorizationã€‚ä½œè€…æå‡ºçš„æ–¹æ³•åœ¨å®ä¾‹åˆ†å‰²ä¸­æ˜¯æ›´æœ‰ä¼˜åŠ¿çš„ã€‚ Instead, our method is based on parallel prediction of masks and class labels, which is simpler and more flexible.In contrast to the segmentation-first level of these methods, Mask R-CNN is based on an instance first strategy. åœ¨ä¸Šé¢ä»‹ç»faster-RCNNæ—¶ï¼Œå·²ç»æåˆ°äº†Mask-RCNNå¢åŠ äº†åˆ†æ”¯ï¼Œæ¥é¢„æµ‹ç‰©ä½“å¯¹åº”çš„æ©è†œ(object mask). åœ¨é˜…è¯»Mask-RCNNçš„æ—¶å€™ï¼Œé‡åˆ°ä¸€ä¸ªé—®é¢˜ã€Œå¦‚ä½•æ¥ç†è§£pixel-to-pixel alignment ã€ we propose a simple, quantiazation-free layer, called ROIAlignï¼Œ that preserves exact spatial locations. faster-RCNNçš„ROI Pooling,ROI Pooling å­˜åœ¨ä¸¤æ¬¡é‡åŒ–ï¼ˆquantizeï¼‰è¿‡ç¨‹ï¼šç¬¬ä¸€æ¬¡æ˜¯å°†å€™é€‰æ¡†çš„è¾¹ç•Œï¼ˆé€šå¸¸æ˜¯æµ®ç‚¹æ•°ï¼‰é‡åŒ–æˆäº†æ•´æ•°ç‚¹åæ ‡ï¼Œç¬¬äºŒæ¬¡æ˜¯å°†é‡åŒ–åçš„è¾¹ç•ŒåŒºåŸŸå¹³å‡åˆ†å‰²æˆkxkä¸ªå•å…ƒbinæ—¶ï¼Œå¯¹æ¯ä¸€ä¸ªå•å…ƒè¿›è¡Œäº†é‡åŒ–ã€‚ä¹Ÿå¯ä»¥ç†è§£ä¸ºã€Œç²—æš´çš„å››èˆäº”å…¥ã€ï¼Œä½¿ç”¨äº†é‚»è¿‘æ’å€¼æ³•ï¼Œä»è€Œé€‰æ‹©ç¦»ç›®æ ‡æœ€è¿‘çš„ç‚¹ã€‚ä½†æ˜¯è¿™ä¹ˆåšä¼šå¸¦æ¥ä¸€å®šçš„åå·®ï¼Œä»è€Œå½±å“åˆ°åˆ†å‰²çš„ç²¾ç¡®ã€‚ä¹Ÿæ˜¯æ–‡ç« ä¸­æåˆ°çš„misalignmentã€‚ã€Œæ”¾å¤§åçš„å›¾æœ‰é©¬èµ›å…‹ï¼Œè€Œç¼©å°çš„å›¾æœ‰å¤±çœŸã€ä¸ºäº†å…‹æœè¿™ä¸€å¼Šç«¯ï¼Œä½œè€…å°±å–æ¶ˆäº†é‡åŒ–çš„è¿‡ç¨‹äº†ï¼Œä½¿ç”¨åŒçº¿æ€§æ’å€¼çš„æ–¹æ³•ã€‚Mask-RCNN ç”¨çš„æ–¹æ³•æ˜¯ROIAlignã€‚ we use bilinear interpolation to compute the exact values of the input features at four regularly sampled locations in each ROI bin, and aggregate the result. å‚è€ƒwikiåŒçº¿æ€§æ’å€¼æŒ‡çš„æ˜¯å¯¹xï¼Œyæ–¹å‘å„è¿›è¡Œä¸€æ¬¡æ’å€¼æ–¹æ³•ã€‚åœ¨åŸå›¾src(source)å’Œç›®æ ‡å›¾dst(destination)ä¸Šè¿›è¡Œå›¾åƒçš„ç¼©æ”¾ã€‚ å……åˆ†åˆ©ç”¨srcä¸­å››ä¸ªçœŸå®çš„åƒç´ å€¼æ¥å…±åŒå†³å®šç›®æ ‡å›¾ä¸­çš„ä¸€ä¸ªåƒç´ å€¼ï¼Œç¼©æ”¾åçš„å›¾åƒè´¨é‡æ›´é«˜ã€‚å‡è®¾srcä¸­å››ä¸ªç‚¹çš„åæ ‡åˆ†åˆ«æ˜¯ï¼ˆ0ï¼Œ0ï¼‰ï¼ˆ0ï¼Œ1ï¼‰ï¼ˆ1ï¼Œ0ï¼‰ï¼ˆ1ï¼Œ1ï¼‰ï¼Œå…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š ROIAlignçš„åšæ³•å¸¦æ¥çš„å¥½å¤„æ˜¯ï¼š 1.it improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics. 2.we found it essential to decouple mask and class prediction: we predict the binary mask for each class independently.]]></content>
      <categories>
        <category>è®ºæ–‡ç¬”è®°</category>
      </categories>
      <tags>
        <tag>å…¥é—¨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inception]]></title>
    <url>%2F2018%2F12%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Inceptionç³»åˆ—æœ‰å››ç¯‡é‡è¦çš„paperï¼Œåˆ†åˆ«æ˜¯ï¼šGoing Deeper with Convolutionsã€Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shifã€Rethinking the Inception Architecture for Computer Visionã€Inception-v4åœ¨æ­¤ï¼Œä¾æ¬¡é˜…è¯»å¹¶åšç¬”è®°ã€‚ GoogleNetIntroduction &amp; MotivationGoing Deeper with Convolutions é¦–æ¬¡æå‡ºäº†ã€ŒInceptionã€æ¨¡å—ä½œä¸ºç½‘ç»œæ„æ¶ï¼Œè¯¥ç½‘ç»œæ„æ¶ä¹Ÿæ˜¯åç»­ä½œä¸ºclassificationå’Œdetectionçš„base networkçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ we introduce a new level of organization in the form of the â€œInception moduleâ€ and also in a more direct sense of increasednetwork depth. ç½‘ç»œçš„sizeä¸»è¦ä»ä¸¤æ–¹é¢è¿›è¡Œè€ƒè™‘ï¼šdepth - the number of levels - of the network å’Œ width - the number of units at each levelã€‚ã€ŒåŠ æ·±ç½‘ç»œdepthã€ã€ã€Œè°ƒèŠ‚è¶…å‚æ•°ã€å¯ä»¥åœ¨recognitionå’Œobject detectionå–å¾—æ›´å¥½çš„æ•ˆæœã€‚ä½†æ˜¯ç½‘ç»œçš„sizeè¿‡å¤§ï¼Œä¼šç›´æ¥å½±å“è¿è¡Œçš„æ€§èƒ½ã€‚å°±åƒä¸€ä¸ªäººè¿‡èƒ–ï¼Œä¼šç›´æ¥å½±å“èº«ä½“å¥åº·ã€‚å½“ç½‘ç»œçš„sizeè¿‡å¤§çš„æ—¶å€™ï¼Œå‚æ•°#paramtersè¿‡å¤šï¼Œæ¶ˆè€—çš„è®¡ç®—èµ„æºå°±è¶Šå¤šï¼Œæ­¤å¤–ï¼Œç‰¹åˆ«æ˜¯åœ¨labeled exampleså¾ˆæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ›´å®¹æ˜“å‡ºç°overfittingã€‚ä¸€èˆ¬æ˜¯é‡‡ç”¨ã€Œdropoutã€æˆ–è€…ã€Œregularizationã€ï¼Œå¹¶ä¸”ã€Œè°ƒæ•´è¶…å‚æ•°ã€å’Œã€Œè®¾ç½®å­¦ä¹ ç‡ã€å»é˜²æ­¢è®­ç»ƒè¿‡ç¨‹ä¸­è¿‡æ‹Ÿåˆç°è±¡çš„å‡ºç°ã€‚ For larger datasets such as Imagenet, deeper architectures are used to get better results and dropout is used to preventoverfitting â€¦â€¦ Since in practice the computational budget is always finite, an efficient distribution of computing resources is preferred to an indiscriminate increase of sizeã€‚ Inception moduleä¸Šé¢çš„æ–¹æ³•æŒºå¥½çš„ï¼Œä½†ä¹ŸæŒºè›®çƒ¦çš„ï¼Œæ‰€ä»¥ä½œè€…è¯•å›¾ç»“åˆã€Œæ•°æ®ç»“æ„ã€ç½‘ç»œç»“æ„ã€æ¥è€ƒè™‘ï¼Œå¦‚ä½•è®¾è®¡ä¸€ä¸ªåˆ›æ–°æ€§çš„architectureï¼Œæ¥æ›´å¥½çš„åˆ©ç”¨è®¡ç®—èµ„æºä»¥åŠç¨å¾®æ”¾å¿ƒã€å¤§èƒ†çš„è®¾ç½®å‚æ•°ä¸€äº›? é¦–å…ˆï¼Œå¼€ä¸ªå°åˆ†æ”¯ï¼Œä»‹ç»ä¸€ä¸‹ã€Œç¨€ç–ç»“æ„ã€çš„ç†è®ºåŸºç¡€ï¼šHebbianåŸç†ã€‚ ä½œè€…ä»neuroscienceçš„è§’åº¦å¾—åˆ°äº†å¯å‘ï¼Œæå‡ºäº†ç½‘ç»œç»“æ„çš„åˆ›æ–°ï¼šã€‚è¯¥åŸç†æŒ‡å‡ºï¼šå„ä¸ªç¥ç»å…ƒæ˜¯ç»„åˆæ•ˆåº”ï¼Œé€šè¿‡ç¥ç»çªè§¦è¿›è¡Œä¿¡æ¯çš„ä¼ é€’ï¼Œå¤§è„‘çš®å±‚æ¥æ”¶ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç¥ç»åå°„æ´»åŠ¨çš„æŒç»­ä¸é‡å¤ä¼šå¯¼è‡´ç¥ç»å…ƒè¿æ¥ç¨³å®šæ€§çš„æŒä¹…æå‡ï¼Œå½“ä¸¤ä¸ªç¥ç»å…ƒç»†èƒAå’ŒBè·ç¦»å¾ˆè¿‘ï¼Œå¹¶ä¸”Aå‚ä¸äº†å¯¹Bé‡å¤ã€æŒç»­çš„å…´å¥‹ï¼Œé‚£ä¹ˆæŸäº›ä»£è°¢å˜åŒ–ä¼šå¯¼è‡´Aå°†ä½œä¸ºèƒ½ä½¿Bå…´å¥‹çš„ç»†èƒã€‚ neurons that fire together, wire together.å°†Fully Connectedå˜ä¸ºç¨€ç–è¿æ¥ï¼ˆsparse connectionï¼‰çš„æ—¶å€™ï¼Œå¯ä»¥åœ¨å¢åŠ ç½‘ç»œæ·±åº¦å’Œå®½åº¦çš„åŒæ—¶å‡å°‘å‚æ•°ä¸ªæ•°ï¼Œä½†æ˜¯å¤§éƒ¨åˆ†çš„ç¡¬ä»¶æ˜¯é’ˆå¯¹å¯†é›†çŸ©é˜µè®¡ç®—ä¼˜åŒ–çš„ï¼Œç¨€ç–çŸ©é˜µè™½ç„¶æ•°æ®é‡å˜å°‘ï¼Œä½†è®¡ç®—æ‰€æ¶ˆè€—çš„æ—¶é—´å¾ˆéš¾å‡å°‘ã€‚GoogleNetå¸Œæœ›åšçš„å°±æ˜¯æ—¢ä¿è¯ç½‘ç»œç»“æ„çš„ç¨€ç–æ€§ã€åˆåˆ©ç”¨å¯†é›†çŸ©é˜µçš„é«˜è®¡ç®—æ€§èƒ½ã€‚ GoogleNetçš„æ ¸å¿ƒæ˜¯Inception moduleï¼Œè€ŒInceptionç›¸å½“äºä¸€ä¸ªConvolutional building blockï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå±€éƒ¨ç¨€ç–æœ€ä¼˜è§£çš„ç½‘ç»œæ„æ¶ï¼Œç„¶åæˆ‘ä»¬åœ¨ç©ºé—´ä¸Šåšå †å ã€‚ä¸‹é¢æˆ‘ä»¬ç»“åˆè®ºæ–‡çš„æ’å›¾æ¥ä»”ç»†åˆ†æä¸€ä¸‹Inception moduleã€‚å›¾aæ˜¯åŸå§‹çš„Inception moduleï¼Œå›¾bæ˜¯å€Ÿé‰´äº†NINï¼ˆNetwork In Networkï¼‰å¼•å…¥1x1çš„å·ç§¯æ“ä½œï¼Œæ”¹è¿›åçš„Inception moduleã€‚ Our network will be built from convolutional building blocks.All we need is to find the optimal local construction and to repeat it spatially. è¾“å…¥æœ‰å››ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šä¸ªå°ºåº¦ï¼ˆ1x1æˆ–3x3æˆ–5x5ï¼‰çš„å·ç§¯å’Œæ± åŒ–è¿›è¡Œç‰¹å¾æå–ã€Œç›¸å½“äºå°†ç¨€ç–çŸ©é˜µåˆ†è§£ä¸ºå¯†é›†çŸ©é˜µã€ï¼Œæ¯ä¸€å°ºåº¦æå–çš„ç‰¹å¾æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œä½†æ˜¯ç»è¿‡ã€Œfilter concatenationã€è¿™æ­¥æ“ä½œåï¼Œè¾“å‡ºçš„ç‰¹å¾ä¸å†æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œç›¸å…³æ€§å¼ºçš„ç‰¹å¾ä¼šè¢«åŠ å¼ºï¼Œè€Œç›¸å…³æ€§å¼±çš„ç‰¹å¾ä¼šè¢«å¼±åŒ–ã€‚è¿™ä¸ªç›¸å…³æ€§é«˜çš„èŠ‚ç‚¹åº”è¯¥è¢«è¿æ¥åœ¨ä¸€èµ·çš„ç»“è®ºï¼Œå³æ˜¯ä»ç¥ç»ç½‘ç»œçš„è§’åº¦å¯¹HebbianåŸç†æœ‰æ•ˆæ€§çš„è¯æ˜ã€Œfilter concatenationã€ï¼Œè¿™ä¸€æ­¥å…¶å®ç›¸å½“äºæ²¿ç€æ·±åº¦æ–¹å‘ï¼ˆæˆ–è€…è¯´åœ¨depthè¿™ä¸ªç»´åº¦ï¼‰è¿›è¡Œæ‹¼æ¥ï¼Œ stack up the first volume to the second volume to make the dimensions match up â€¦â€¦ Output a single output vector forming the input ofnext stageã€‚ ç»“åˆUdacityè§†é¢‘å’Œcodeæ¥åŠ æ·±ä¸€ä¸‹å¯¹ã€Œfilter concatenationã€çš„ç†è§£ concatenated_tensor = tf.concat(3,[branch1, branch2, branch3, branch 4]) E.g:{General}ï¼šè¾“å…¥ 28x28x192 volume ï¼Œå¹¶åˆ—ç»è¿‡ 1x1å·ç§¯æ“ä½œã€3x3å·ç§¯æ“ä½œã€5x5å·ç§¯æ“ä½œã€max-poolï¼Œåˆ†åˆ«å¾—åˆ°28x28x64ã€28x28x128ã€28x28x32ã€28x28x32 volume, å°†å¹¶åˆ—çš„volumeæ²¿ç€æ·±åº¦æ–¹å‘è¿›è¡Œæ‹¼æ¥ï¼Œè¾“å‡º 28x28x256 volumeã€‚ Feifei-Liçš„cs231nçš„è¯¾ä»¶é‡Œæ˜¯æè¿°CNNçš„ï¼ševery layer of a ConvNet transforms one volume of activations to another through a differentiable function.We use three main types of layers to build ConvNet architectures:Convolutional Layer, Pooling Layer,and Fully-Connected Layer. Conv layer will compute the output of neurons that are connected to local regions in the input,each computing a dot product between their weights and a small region they are connected to the input volume.Pool layer will perform a downsampling operation along the spatial dimensions(width,height)FC layer will compute the class score,resulting in volume of sizeã€Œ1x1x#classã€ã€‚ {Specific}ï¼š5x5çš„å·ç§¯æ“ä½œå¾—åˆ°äº†28x28x32çš„blockã€‚filter size =5x5x192ï¼Œ5 pixels width and height, 192 pixels depthï¼ˆfilterçš„æ·±åº¦éœ€è¦å’Œå‰ä¸€feature mapçš„æ·±åº¦ä¿æŒä¸€è‡´ã€‚ï¼‰ è®¾input volume width = W, the width of receptive field = F_w, zero padding on the border = P, stride = Sé‚£ä¹ˆoutput volume width = (W-F+2P)/S+1ã€‚åŒç†ä¹Ÿå¯ä»¥å¾—åˆ°output volume heightã€‚æ­¤å¤–, input volume depth = D1æ­¤å¤–ï¼Œè¢«filterè¦†ç›–çš„å›¾åƒåŒºåŸŸç§°ä¸ºreceptive fieldï¼Œå…·ä½“æ“ä½œæ˜¯ï¼šslide each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any positionï¼Œå³filterä¸­çš„å€¼å’ŒåŸå§‹å›¾åƒä¸­receptive fieldä¸­çš„åƒç´ å€¼è¿›è¡Œç‚¹ç§¯è¿ç®—ï¼Œäº§ç”Ÿactivation mapæˆ–feature mapã€‚å›¾åƒä¸€èˆ¬éƒ½æ˜¯å±€éƒ¨ç›¸å…³çš„ï¼Œç¬¬n+1å±‚çš„æ¯ä¸ªç¥ç»å…ƒå’Œç¬¬nå±‚çš„receptive fieldä¸­çš„ç¥ç»å…ƒè¿æ¥ï¼Œè€Œä¸éœ€è¦å’Œç¬¬nå±‚çš„æ‰€æœ‰ç¥ç»å…ƒè¿æ¥ï¼ŒConvNetå…·æœ‰local connectivity(å±€éƒ¨è¿æ¥) çš„æ€§è´¨ã€‚å½“filterçš„receptive fieldè¶Šå¤§ï¼Œfilterèƒ½å¤Ÿå¤„ç†çš„åŸå§‹è¾“å…¥å†…å®¹çš„èŒƒå›´å°±è¶Šå¤§ã€‚éšç€ç»è¿‡æ›´å¤šçš„å·ç§¯å±‚ï¼Œå¾—åˆ°çš„æ¿€æ´»æ˜ å°„ä¹Ÿå°±å…·æœ‰æ›´ä¸ºå¤æ‚çš„ç‰¹å¾ã€‚ è®¾ number of filters = K, ä¹Ÿæ˜¯output volume depthçš„å€¼ã€‚å½“filterçš„æ•°ç›®è¶Šå¤šï¼Œspatial dimensionså°±ä¼šä¿ç•™çš„è¶Šå¥½ã€‚CNNå…·æœ‰local connectionå’Œparameter sharingçš„ç‰¹ç‚¹ã€‚æ¯ä¸ªfilterçš„æƒé‡çš„ä¸ªæ•° = F_w x F_h x D1, æ€»çš„æƒé‡ä¸ªæ•°= F_w x F_h x D1 x K æˆ‘ä»¬å†åˆ†æä¸€ä¸‹compution cost cs231n æŒ‡å‡ºï¼š the largest bottleneck to be aware of when constructing the ConvNet is the memory bottle neck.we need to keep track of the intermediate volume size, the paramter size and the memory.Reference:cs231n ç°åœ¨æˆ‘ä»¬æ¥åˆ†æä¸€ä¸‹ï¼Œä¸Šé¢çš„å›¾bç›¸æ¯”å›¾açš„ä¼˜åŠ¿åœ¨å“ªé‡ŒğŸ§ã€‚1x1çš„å·ç§¯æ˜¯ä½œä¸ºç“¶é¢ˆå±‚çš„ä½œç”¨ï¼Œç”¨å¾ˆå°çš„è®¡ç®—é‡å¯ä»¥å¢åŠ ä¸€å±‚ç‰¹å¾å˜æ¢å’Œéçº¿æ€§å˜æ¢ã€‚æ­¤å¤–ï¼Œä¸€èˆ¬æ¶‰åŠåˆ°æ”¹å˜é€šé“æ•°ï¼Œéƒ½ä¼šä½¿ç”¨1x1å·ç§¯æ“ä½œï¼Œä¾‹å¦‚æ®‹å·®è¿æ¥å’ŒDenseè¿æ¥ the bottleneck is usually the smallest part of somethingæˆ‘ä»¬æ¥è®¡ç®—ä¸€ä¸‹å›¾aä¸­5x5çš„å·ç§¯æ“ä½œå¾—åˆ°äº†28x28x32çš„blockçš„æ—¶å€™ï¼Œæ‰€éœ€è¦çš„multiplesçš„æ¬¡æ•°ã€‚ä»¥åŠå›¾bä¸­å…ˆä½¿ç”¨1x1çš„å·ç§¯æ“ä½œå…ˆå¾—åˆ°28x28x16ï¼Œå†ä½¿ç”¨5x5çš„å·ç§¯æ“ä½œå¾—åˆ°äº†28x28x32çš„blcokçš„æ—¶å€™ï¼Œæ‰€éœ€è¦çš„multiplesçš„æ¬¡æ•°ã€‚ 1.å›¾a (28x28x32) x (5x5x192) = 120million ã€Œä¸€ä¸ªoutput volumeæ‰€éœ€è¦çš„ä¹˜ç§¯æ¬¡æ•° x the number of output valuesã€ 2.å›¾b ï¼ˆ28x28x16) x (1x1x192) + (28x28x32) x (5x5x16) = 12.4 million ä»ä¸Šé¢ğŸ‘†ä¸¤ä¸ªå¯¹æ¯”å¯ä»¥çŸ¥é“1x1çš„å·ç§¯æ“ä½œå¤§å¤§çš„å‡å°‘äº†è®¡ç®—é‡ã€‚ GoogleNetâ€™s architectureé¦–å…ˆï¼Œä¸ºäº†æœ‰ä¸€ä¸ªåˆæ­¥çš„å°è±¡ï¼Œå…ˆæˆªå–äº†GoogleNetçš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°è¿™é‡Œæœ‰ä¸€ä¸ªã€Œsoftmaxã€çš„åˆ†æ”¯ï¼Œæ•´ä¸ªç»“æ„ä¸­æœ‰ä¸¤ä¸ªã€Œsoftmaxã€ï¼Œå®ƒç›¸å½“äºè¾…åŠ©åˆ†ç±»å™¨ï¼Œç»“åˆcodeæˆ‘ä»¬å¯ä»¥çŸ¥é“è¯¥æ“ä½œæ˜¯å°†ä¸­é—´æŸä¸€å±‚çš„è¾“å‡ºç”¨ä½œåˆ†ç±»ï¼Œå¹¶æŒ‰ä¸€ä¸ªè¾ƒå°çš„æƒé‡ï¼ˆ0.3ï¼‰åŠ åˆ°æœ€ç»ˆåˆ†ç±»ç»“æœä¸­èµ·åˆ°çš„æ˜¯æ¢¯åº¦å‰å‘ä¼ è¾“çš„ä½œç”¨ã€‚è®ºæ–‡é‡Œæ˜¯è¿™ä¹ˆäº¤ä»£çš„ï¼š By adding auxiliary classifiers connected to these intermediate layers, we would expect to encourage discrimination in the lower stages in the classifier, increase the gradient signal that gets propagated back, and provide additional regularization. å…¶æ¬¡ï¼Œä¸ºäº†å¯¹GoogleNetçš„ç»„æˆæœ‰ä¸€ä¸ªæ¦‚å¿µï¼Œå¼•ç”¨äº†è®ºæ–‡ä¸­çš„è¡¨æ ¼ï¼šä¸Šé¢è®¨è®ºæ—¶ï¼Œå·²ç»è¯´è¿‡GoogleNetæ˜¯æ¨¡å—åŒ–çš„ï¼Œå †å äº†å¤šä¸ªInception Moduleï¼Œé åçš„Inception Moduleèƒ½å¤ŸæŠ½å–æ›´é«˜é˜¶çš„æŠ½è±¡çš„ç‰¹å¾ã€‚ Batch NormalizationInternal covariate shiftâ€œInternalâ€æŒ‡çš„æ˜¯ç¥ç»ç½‘ç»œçš„éšå«å±‚ï¼Œâ€Covariateâ€æŒ‡çš„æ˜¯è¾“å…¥çš„æƒé‡å‚æ•°åŒ–ï¼Œâ€œInternal Covariate Shiftâ€æŒ‡çš„æ˜¯åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œè¾“å…¥çš„æ¦‚ç‡åˆ†å¸ƒä¸å›ºå®šï¼Œç½‘ç»œçš„å‚æ•°åœ¨ä¸æ–­çš„å˜åŒ–ï¼Œç¥ç»ç½‘ç»œçš„éšå«å±‚ä¹Ÿè¦ä¸æ–­çš„å»ã€Œé€‚åº”ã€æ–°çš„åˆ†å¸ƒã€‚è¿™ä¸ªç°è±¡ä¼šè®©æ¨¡å‹æ›´åŠ éš¾è®­ç»ƒï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦æ›´åŠ è°¨æ…çš„åˆå§‹åŒ–æ¨¡å‹å‚æ•°å’Œå­¦ä¹ ç‡ã€‚å› æ­¤ä½œè€…å¼•å…¥äº†Normalization æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ Normalizationé€šè¿‡è§„èŒƒåŒ–çš„æ‰‹æ®µï¼Œå°†æ¯å±‚ç¥ç»ç½‘ç»œä»»æ„ç¥ç»å…ƒè¿™ä¸ªè¾“å…¥å€¼çš„åˆ†å¸ƒâ€œå¼ºè¡Œæ‹‰å›â€åˆ°å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1çš„åˆ†å¸ƒä¸­.å¸¸è§„çš„æ­£åˆ™åŒ–å…¬å¼ä¸ºï¼š$\hat{x}^{k}=\frac{x^{k}-E(x^{k})}{\sqrt{var(x^{k})}}$è¿™ä¹ˆåšçš„ä¼˜ç‚¹æ˜¯å¯ä»¥åŠ å¿«æ”¶æ•›çš„é€Ÿåº¦ï¼Œä½†æ˜¯ç¼ºç‚¹æ˜¯å‡å¦‚è¯¥å±‚çš„å„ä¸ªç‰¹å¾äº’ä¸ç›¸å…³ï¼Œç®€å•çš„æ­£åˆ™åŒ–æ“ä½œå¯èƒ½ä¼šæ”¹å˜è¯¥å±‚çš„ç‰¹å¾è¡¨è¾¾ã€‚ä¸ºäº†ç¡®ä¿ç¥ç»ç½‘ç»œé‡Œé¢å¯ä»¥è¿›è¡Œæ’ç­‰å˜æ¢(identity transform)ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å¸¸è§„çš„æ­£åˆ™åŒ–å…¬å¼è¿›è¡Œæ”¹å˜ã€‚ç”±æ­¤æˆ‘ä»¬ä¹Ÿå¼•å…¥äº†Batch Normalization:$BN_{\gamma,\beta}$ã€‚ $y^{k}=\gamma^{k}\hat{x^{k}}$;$\beta^{k}$å…¶ä¸­æ¯ä¸€ä¸ªactivationéƒ½ä¼šå¼•å…¥ä¸¤ä¸ªè¶…å‚æ•°$\gamma,\beta$ã€‚è¿™ä¸¤ä¸ªå‚æ•°ä¹Ÿæ˜¯ç¥ç»ç½‘ç»œéœ€è¦å­¦ä¹ çš„å‚æ•°ï¼Œåˆ†åˆ«èµ·åˆ°çš„æ˜¯scaleå’Œshiftçš„ä½œç”¨ã€‚ç›¸å½“äºåœ¨åŸæ¥æ­£åˆ™åŒ–çš„åŸºç¡€ä¸Šï¼Œå†è¿›è¡Œäº†ä¸€æ¬¡çº¿æ€§å˜åŒ–ã€‚åœ¨mini-batchçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒBNæ˜¯è§„èŒƒåŒ–äº†æ¯ä¸€å±‚çš„è¾“å…¥ï¼Œ$z=g(BN(W,u))$ ã€Œgè¡¨ç¤ºçš„éçº¿æ€§æ“ä½œï¼Œä¾‹å¦‚ï¼šreluã€ï¼Œä»è€Œå‡å°‘äº†Internal Covariate shiftçš„å¹²æ‰°ã€‚ the inputs to each layer are affected by the parameters of all preceding layers - so that smallchanges to the network parameters amplify as the network becomes deeper â€¦â€¦ Fixed distribution of inputs to asub-network would have a positive consequences for the layers outside the network, as well. æ­¤å¤–ï¼ŒBNçš„åˆ›æ–°ä¹Ÿåœ¨äºapplied to the sub-networks and layers, incorporate the normalization in the network architecture as wellã€‚ Batch Normalizationä¸ä»…å…è®¸ä¸é‚£ä¹ˆè°¨æ…çš„åˆå§‹åŒ–ï¼Œè¿˜å…è®¸ä½¿ç”¨æ›´é«˜çš„learning rateã€‚ Rethinking the Inception Architecture for Computer visionå› ä¸ºè®¡ç®—å¼€é”€ã€å‚æ•°é‡é™åˆ¶äº†æŠŠInceptionéƒ¨ç½²åˆ°ç§»åŠ¨ç«¯å’Œä¸€äº›åœºæ™¯ä¸­ï¼Œåœ¨abstracté‡Œï¼Œä½œè€…æŒ‡å‡ºäº†å¯¹ç½‘ç»œæ„æ¶è¿›è¡Œæ”¹è¿›çš„æ€è·¯ã€‚ Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by factorized convolutions and aggressive regularization. å·ç§¯æ ¸çš„å› å¼åˆ†è§£Paperé‡Œæ¢ç´¢äº†å‡ å¼ å°†è¾ƒå¤§çš„å·ç§¯æ ¸åˆ†è§£ä¸ºè¾ƒå°çš„å·ç§¯æ ¸çš„è®¾ç½®æ–¹å¼ã€‚ä¾‹å¦‚ï¼š5x5çš„å·ç§¯æ ¸æ›¿æ¢ä¸ºä¸¤ä¸ª3x3çš„å·ç§¯æ ¸ï¼›3x3çš„å·ç§¯æ ¸æ›¿æ¢ä¸º1x3å’Œ3x1çš„å·ç§¯æ ¸ã€‚è¿™æ ·å…·æœ‰ç›¸åŒçš„receptive fieldçš„åŒæ—¶å¯ä»¥å¤§å¤§çš„å‡å°è®¡ç®—å¼€é”€ã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ã€Œnxnçš„å·ç§¯è¢«æ›¿æ¢ä¸º1xnå’Œnx1çš„å·ç§¯ã€çš„è¿™ç§ç©ºé—´ä¸Šåˆ†è§£ä¸ºéå¯¹ç§°å·ç§¯çš„åšæ³•åœ¨å‰é¢å‡ å±‚layerçš„æ•ˆæœä¸æ˜¯å¾ˆå¥½ï¼Œæ›´é€‚ç”¨äºä¸­ç­‰è§„æ ¼å¤§å°çš„feature mapï¼Œï¼ˆmçš„èŒƒå›´ä»12åˆ°20ï¼‰ã€‚æ­¤å¤–æˆ‘ä»¬è¿˜è¦æ€è€ƒğŸ¤”å‡ ä¸ªé—®é¢˜ã€‚1.ç”¨å°å·ç§¯æ ¸æ›¿æ¢å¤§å·ç§¯æ ¸ï¼Œæ˜¯å¦ä¼šå¸¦æ¥ä¿¡æ¯æŸå¤±(loss of expressiveness)? ä¸ä¼šï¼Œåªè¦å¤šæ¬¡å åŠ çš„å°å·ç§¯æ ¸å’Œå¼€å§‹çš„å¤§å·ç§¯æ ¸å…·æœ‰ç›¸åŒçš„receptive fieldã€‚2.å¦‚æœæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯¹è®¡ç®—å¼€é”€ä¸­çš„çº¿æ€§éƒ¨åˆ†è¿›è¡Œå› å¼åˆ†è§£ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆä¸ç›´æ¥åœ¨ç¬¬ä¸€æ¬¡ä¿æŒçº¿æ€§æ¿€æ´»ï¼ˆlinear activationï¼‰ï¼Ÿå› ä¸ºåœ¨å®éªŒä¸­è¡¨æ˜ï¼Œéçº¿æ€§æ¿€æ´»æ€§èƒ½æ›´å¥½ã€‚ Label Smoothing Regularizationå› ä¸ºå¤§å¤šæ•°çš„æ•°æ®é›†éƒ½å­˜åœ¨é”™è¯¯çš„æ ‡ç­¾ï¼Œ ä½†æ˜¯minimize the cost function on the wrong labels can be harmfulã€‚å› æ­¤åœ¨Model Regularizationä¸­ï¼Œå¯ä»¥é€šè¿‡åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ä¸»åŠ¨åŠ å…¥å™ªå£°ä½œä¸ºpenaltyï¼Œè¿™æ ·çš„æ¨¡å‹å…·æœ‰noise Robustnessã€‚Label Smoothing Regularization(LSR)æ˜¯å…¶ä¸­çš„ä¸€ç§regularizationçš„æ–¹æ³•ã€‚ Here we propose a mechanism to regularize the classifier layer by estimating the marginalized effect of label-dropout duringtraining. {ä¸¾ä¸€ä¸ªUniversity of Waterlooçš„WAVE LABçš„ ME 780ä¸­lecture 3ï¼šRegularization for deep modelsçš„ä¾‹å­æ¥å¸®åŠ©ç†è§£ï¼šground-truth: y1_label=[1,0,0,â€¦â€¦ï¼Œ0]prediction: ç»è¿‡softmax classifierå¾—åˆ°çš„softmax output: y1_out=[0.87,0.001,0.04â€¦â€¦,0.03]. } maximum likelihood learning with softmax classifier and hard targets may actually never converge, the softmax cannever predict a probability of exactly 0 or 1, so it will continue to learn larger and larger weights, making moreextreme predictions. å‡è®¾xä¸ºtraining exampleï¼Œ$p(k|x)$ä¸ºxå±äºã€Œlabel kã€çš„æ¦‚ç‡ï¼Œ$q(k|x)$ä¸ºxå±äºã€Œground-truth labelã€çš„æ¦‚ç‡ã€‚ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œå¿½ç•¥äº†på’Œqåœ¨example xä¸Šçš„ç›¸å…³æ€§ã€‚ ç›®æ ‡å‡½æ•°ï¼šã€Œæœ€å°åŒ–äº¤å‰ç†µã€ã€‚å› ä¸ºäº¤å‰ç†µè¡¡é‡çš„æ˜¯ä¸¤ä¸ªåˆ†å¸ƒï¼ˆpå’Œqï¼‰çš„ç›¸ä¼¼æ€§ï¼Œæœ€å°åŒ–ç›®æ ‡å‡½æ•°æ˜¯ä¸ºäº†è®©é¢„æµ‹çš„labelæ¦‚ç‡åˆ†å¸ƒ$p(k|x)$ï¼ˆå³ä¾‹å¦‚ä¸Šé¢çš„softmaxçš„è¾“å‡ºï¼‰å’Œground-truth labelçš„æ¦‚ç‡åˆ†å¸ƒ$q(k|x)$å°½å¯èƒ½çš„æ¥è¿‘ã€‚ã€Œæœ€å°åŒ–äº¤å‰ç†µã€ä¹Ÿç­‰ä»·ä¸ºã€Œæœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°ã€ã€‚ä½†æ˜¯æˆ‘ä»¬éœ€è¦å¯¹è¿™ä¸ªç›®æ ‡å‡½æ•°è¿›è¡Œäº†æ”¹è¿›ã€‚å› ä¸ºåœ¨å•ç±»æƒ…å†µä¸‹ï¼Œå•ä¸€çš„äº¤å‰ç†µå¯¼è‡´æ ·æœ¬å±äºæŸä¸ªç±»åˆ«çš„æ¦‚ç‡éå¸¸å¤§ï¼Œæ¨¡å‹å¤ªè¿‡ä¸è‡ªä¿¡è‡ªå·±çš„åˆ¤æ–­ã€‚è¿™æ ·ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œæ­¤å¤–è¿˜ä¼šé™ä½æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚ä¸ºäº†é¿å…æ¨¡å‹è¿‡äºè‡ªä¿¡ï¼Œå¼•å…¥äº†ä¸€ä¸ªç‹¬ç«‹äºæ ·æœ¬åˆ†å¸ƒçš„å˜é‡u(k)ï¼Œè¿™ç›¸å½“äºåœ¨ground-truth distributionä¸­åŠ å…¥äº†å™ªå£°ï¼Œç»„æˆä¸€ä¸ªæ–°çš„åˆ†å¸ƒã€‚åœ¨å®éªŒä¸­ï¼Œä½¿ç”¨çš„æ˜¯å‡åŒ€åˆ†å¸ƒ(uniform distribution)ä»£æ›¿äº†u(k) we propose a mechanism for encouraging the model to be less confident. While this may not be desired if the goal is to maximize the log-likelihood of training labels, it does regularize the model and makes it more adaptable â€¦â€¦ we refer to thischange in ground-truth label distribution as label-smoothing regularization, or LSR. Resnetintroductionä¸€èˆ¬æ¥è¯´ï¼Œæ¨¡å‹çš„æ·±åº¦åŠ æ·±ï¼Œå­¦ä¹ èƒ½åŠ›å¢å¼ºï¼Œä½†æ˜¯ä¸èƒ½ç®€å•çš„å¢åŠ ç½‘ç»œçš„æ·±åº¦ï¼Œå¦åˆ™ä¼šå‡ºç°éšç€ç½‘ç»œæ·±åº¦å¢åŠ ï¼Œtraining errorå’Œtest errorå˜é«˜çš„ç°è±¡ã€‚è¿™ä¹Ÿè¯´æ˜ç½‘ç»œç»“æ„å˜å¤æ‚æ—¶ï¼Œoptimizationå˜å¾—æ›´åŠ å›°éš¾ã€‚Kaiming Heæå‡ºäº†æ·±åº¦æ®‹å·®å­¦ä¹ ï¼ˆdeep residual learning frameworkï¼‰,é€šè¿‡ç½‘ç»œç»“æ„çš„åˆ›æ–°æ¥æœ‰æ•ˆçš„è§£å†³äº†ä¸Šè¿°çš„æ¢¯åº¦å¼¥æ•£ç°è±¡(degradation)ã€‚ ç½‘ç»œç»“æ„ ä»ä»¥ä¸‹ä¸¤ç‚¹æ¥åˆ†æç½‘ç»œç»“æ„çš„åˆ›æ–° shortcut connectionæ®‹å·®çš„ç½‘ç»œç»“æ„æ˜¯å‰å‘ç¥ç»ç½‘ç»œ+shortcutã€‚ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºåœ¨å·²æœ‰çš„ç½‘ç»œç»“æ„ä¸­å¢åŠ äº†ä¸€ä¸ªbranchï¼Œèµ·åˆ°çš„æ˜¯æ’ç­‰æ˜ å°„ï¼ˆidentity mappingï¼‰çš„ä½œç”¨ï¼Œè¿™æ ·ä¿è¯äº†ä¸€ä¸ªæ·±åº¦æ¨¡å‹çš„training erroræœ€èµ·ç ä¿è¯shallow counterpartæ¨¡å‹çš„training erroræ˜¯ä¸€è‡´çš„ï¼Œé”™è¯¯ç‡ä¸ä¼šé«˜äºæµ…å±‚ã€‚æ­¤å¤–ï¼Œè¿™ç§åšæ³•æ—¢ä¸ä¼šå¢åŠ é¢å¤–çš„å‚æ•°ä¹Ÿä¸ä¼šå¢åŠ é¢å¤–çš„è®¡ç®—é‡ã€‚ residual representationsä¸¤ç§å‡½æ•°çš„è¡¨è¾¾æ•ˆæœæ˜¯ç›¸åŒçš„ï¼Œä½†æ˜¯ä¼˜åŒ–çš„éš¾åº¦æ˜¯ä¸åŒçš„ã€‚å¯¹æ®‹å·®è¿›è¡Œæ‹Ÿåˆæ˜¾ç„¶è¦æ›´åŠ å®¹æ˜“ã€‚æ®‹å·®å‡½æ•° $F(x):=H(x)-x$ã€‚å¼•å…¥æ®‹å·®åçš„æ˜ å°„å¯¹è¾“å‡ºçš„å˜åŒ–æ›´åŠ æ•æ„Ÿï¼Œå¦‚H(5)=5.1,F(5)=0.1, è¾“å‡ºä»5.1å˜åŒ–åˆ°5.2ï¼Œå¢åŠ çš„å¹…åº¦ä¸º2%ï¼Œä½†æ˜¯æ®‹å·®æ˜¯ä»0.1å˜åŒ–åˆ°0.2ï¼Œå¢åŠ å¹…åº¦ä¸º100%ã€‚æ®‹å·®çš„æ€æƒ³å°±æ˜¯å»é™¤ç›¸åŒçš„ä¸»ä½“éƒ¨åˆ†ï¼Œçªå‡ºå¾®å°çš„å˜åŒ–ã€‚ è®¡ç®—å…¬å¼$${y}=F({x},{W_i})+W_s{x}$$å…¶ä¸­$F({x},{W_i})$å’Œ$x$æ˜¯éœ€è¦å­¦ä¹ çš„æ®‹å·®æ˜ å°„ï¼Œåœ¨è®¡ç®—çš„æ—¶å€™éœ€è¦ä¿è¯ç»´æ•°ä¸€è‡´ï¼Œè€ƒè™‘åˆ°åœ¨ä¸¤ä¸ªfeature map ä¸­è¿›è¡Œé€å…ƒç´ ç›¸åŠ ï¼ˆelement-wise addition)ã€‚å½“ç»´æ•°ä¸ä¸€è‡´çš„æ—¶å€™ï¼Œé€šè¿‡å¼•å…¥çº¿æ€§æ˜ å°„W_sæ¥åŒ¹é…ç»´åº¦ æœ€ç»ˆå–å¾—çš„æ•ˆæœæ˜¯ï¼š these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. Inception-Resnetå› ä¸ºResidual connectionsåœ¨è®­ç»ƒæ·±åº¦ç½‘ç»œæ—¶ï¼Œååˆ†æœ‰ä¼˜åŠ¿ï¼Œå†åŠ ä¸Šinceptionç½‘ç»œçš„æ·±åº¦ä¹Ÿæ¯”è¾ƒæ·±ï¼Œæ‰€ä»¥ä½œè€…å°è¯•å°†ä¸¤ç§æ–¹æ³•ç»“åˆèµ·æ¥ã€‚æ›´ç¡®åˆ‡çš„è¯´æ˜¯ï¼Œå°†Inceptionä¸­çš„filter concatenationä¸­çš„ä¸€éƒ¨åˆ†æ›¿æ¢ä¸ºresidual connectionsçš„ç»“æ„ã€‚ç»“åˆäº†residual connectionçš„inceptionæ¨¡å—å¦‚ä¸‹æ‰€ç¤ºï¼š ä½†æ˜¯å¼•å…¥residual connectionåï¼Œç½‘ç»œå¤ªæ·±ï¼Œç¨³å®šæ€§ä¸å¥½ï¼Œå› æ­¤å†æ¬¡åšäº†ä¿®æ”¹ã€‚å›¾ä¸­inceptionæ¡†å¯ä»¥ç”¨ä»»æ„å…¶ä»–subnetworkæ›¿ä»£ï¼Œä½†æ˜¯è¿™æ¬¡ä¿®æ”¹æ˜¯åœ¨è¾“å‡ºåå¼•å…¥äº†ç¼©æ”¾ç³»æ•°(scale),å†ç›¸åŠ å’Œæ¿€æ´»ã€‚æ–‡ç« æå‡ºäº†ä¸¤ä¸ªç‰ˆæœ¬ï¼šInception-ResNet v1å’ŒInception-ResNet v2ï¼Œç›¸æ¯”åŸæ¥ï¼ŒåŠ å¿«è®­ç»ƒçš„æ”¶æ•›é€Ÿåº¦ã€‚åœ¨å›¾åƒè¯†åˆ«ï¼Œè§†é¢‘æ£€æµ‹ç­‰é¢†åŸŸéƒ½ä½œä¸ºäº†base-networkã€‚ In the experimental section we demonstrate that it is not very difficult to train competitive very deep networks without utilizing residual connections. However the use of residual connections seems to improve the training speed greatly, which is alone a great argument for their use.]]></content>
      <categories>
        <category>è®ºæ–‡ç¬”è®°</category>
      </categories>
      <tags>
        <tag>å…¥é—¨</tag>
      </tags>
  </entry>
</search>
